\documentclass[a4paper,10pt]{report}
\usepackage[utf8]{inputenc}

%multiline comments
\usepackage{verbatim}

%derivatives, ...
\usepackage{physics}

\usepackage{natbib}
\bibliographystyle{plainnat2} 
\setcitestyle{round, comma}

%for images
\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{shapes,snakes}
\usetikzlibrary{backgrounds}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{math}

\usepackage{subfig}

%math symbols
\usepackage{amsmath}
\usepackage{amssymb}

%SI package
\usepackage{siunitx}

\usepackage{geometry}
 \geometry{
 a4paper,
 total={160mm,227mm},
 left=30mm,
 right=30mm,
 top=30mm,
 }

%code listing
\usepackage{listings}

%linked toc
\usepackage[bookmarks]{hyperref}

%tables
\usepackage{booktabs}

\usepackage{enumitem}
\setlist[itemize]{align=parleft,left=0pt..1em}

\usepackage{bm}

\usepackage{dirtree}



%setting for lst
\def\lstbasicfont{\fontfamily{pcr}\selectfont\footnotesize}
\lstset{%
  basicstyle={\lstbasicfont},                 % set font
  showstringspaces=false,                     % do not emphasize spaces in strings
  tabsize=4,                                  % number of spaces of a TAB
  mathescape=false,escapechar=§,              % escape to latex with §...§
  upquote=true,                               % upright quotes
  aboveskip={1.5\baselineskip},               % a bit of space above listings
  columns=fixed,                              % nice spacing
  %
  % the following is for replacing some matlab relations like >= or ~=
  % by the corresponding LaTeX symbols, which are much easier to read ...
  literate=%
    {~}{{$\neg$}}1 %            \neg
    {<=}{{\tiny$\leq$}}1 %      \leq
    {>=}{{\tiny$\geq$}}1 %      \geq
    {~=}{{\tiny$\neq$}}1 %      \neq
    {delta}{{\tiny$\Delta$}}1%  \Delta
    {iend}{{\fontfamily{pcr}\selectfont end}}3% end when indexing matrix elements
}

\lstset{
    numbersep=3mm, numbers=left, numberstyle=\tiny, % number style
  }

  \lstset{%
    frame=single,                             % frame
  }
    \lstset{%
      framexleftmargin=6mm, xleftmargin=6mm   % tweak margins
    }


%opening
\title{Design of a machine learning model for the characterisation of young planets from dust morphologies in discs}
\author{Alessandro Ruzza}

\begin{document}

\input{copertina/Frontespizio_UniMi.tex}

\addcontentsline{toc}{chapter}{Abstract}
\begin{abstract}
    Protoplanetary discs present substructures,
    such as axisymmetric regions of luminosity depletion (gaps),
    that can be explained by the presence of forming planets. 
    Features of these objects can be inferred from their
    observation and analysis. A remarkable example is the 
    estimation of the planetary mass from the gaps morphology.
    The approaches currently used, empirical formulae or numerical
    simulations, are both limited in precision or applicability.
    In this thesis we propose a machine learning approach:
    using a neural network to infer this information from 
    disk images with the requirement of the least amount
    of physical features not directly observable.
    Possible future developments of such models require 
    data for the train and test phases.
    We design and build a database for this purpose collecting 
    data obtained from numerical simulations and providing an 
    easy-to-use interface for the implementation of machine learning
    models using TensorFlow libraries.
\end{abstract}

\tableofcontents

\chapter{Introduction}

The question on the origin of Earth and solar system's planets has motivated 
thinkers and scientists for a long time.
Back to the eighteenth century, Kant and Laplace formulated the
\emph{Nebula Hypothesis} proposing that the solar system
formed from a primordial rotating disc of gas and dust. Quantitative descriptions followed in the upcoming centuries 
gradually developing on this idea the modern theory of planet formation.

Despite the long history, the first extrasolar planet, 51 Pegasi b, was only detected in 1995 using doppler spectroscopy to
measure the star radial velocity. Figure 1.1 shows the original data obtained in this first observation \citep{Mayor1995}
interpolated with the sine curve that indicates the star motion around the centre of mass of the star-planet system.
In more recent times, with breakthroughs in interferometric observations 
and the advent of new telescopes, such as the Atacama Large Millimiter Array which became operative in 2013, we were also
finally able to obtain high resoluted images of planet's
cradles, protoplanetary discs.

Since their first observations over 4000 exoplanets and hundreds of protoplanetary discs have been observed in an extraordinary 
diversity of properties and morphologies, 
offering a precious statistically valid pool of data to evaluate and enhance
the theoretical picture on planet formation.

\begin{figure}
    \begin{center}
        \raisebox{+0.06\height}{\includegraphics[scale=0.73]{images/51pegasib.png}}
    \scalebox{0.78}{\input{images/1.1.exoplanets/exop2.pgf}}
    \end{center}
    \caption{Left panel: first observation of an extrasolar planet (51 Pegasi b) with the radial-velocity method, the graph shows the orbital motion 
    of 51 Pegasi obtained from the data. Reprinted from \citet{Mayor1995}. Right panel:
    mass vs semi-major orbital axis of confirmed exoplanets up to September 2021. The colors 
    mark the various methods used for their detection. Data taken from exoplanet.eu.}
\end{figure}

Many techniques have been proposed and developed to interpret these new observations,
which exploit measures of the discs' radiative emission and analysis of the morphologies revelead
with the aim of determining their properties and detecting the presence of young planets.
Gaps, i.e. annular regions of luminosity depletion, can, for example, be explained 
by the presence of embedded interacting planets whose properties, such as their mass, are
tied to the width and depth of these substructures \citep{gap_opening1,gap_opening2,gap_opening3}.

To exploit this relationship and more generally for the characterisation of protoplanetary discs
two approaches are currently used. Some variables can be linked with empirical relationships 
inferred from data,
for example, \citet{Lodato_2019} and \citet{kanagawa} proposed 
two different formulae which allow estimating planet's masses measuring
the width of the induced gap. These relations are usually simple polynomial or power laws, which 
allow a fast computation of unknown properties and can be easily explained with theoretical
arguments but, on the other end,
lack in accuracy and precision requiring the use of more advanced tools.

Numerical simulations are today's response to these needs. Sophisticated codes,
 like \lstinline{PHANTOM} \citep{phantom} and \lstinline{MCFOST} \citep{mcfost1,mcfost2}, 
are able to apply the physical models that describe the dynamics, particle interaction and radiative emission at a small scale 
to simulate the overall disc evolution and synthetize images reproducing hypothetical observations which would be 
obtained if the system was real.
These results can thus be directly compared  with observations in the data 
space to check the assumptions made to start the simulation and 
search the parameters which provide the best fit.
With this method, the masses of some planets embedded 
in discs' gaps were measured with relatively low uncertainty.
In one of the first applications of this technique, \citet{Dipierro_2015} studied
the high resolution observation of HL Tau 
and were able most of the observed features 
simulating three embedded planets with masses of 0.2,  0.27  and  0.55 $M_{Jup}$.
The comparison between the observed image of the dust continuum emission at 1.3 mm and the simulated observation
is shown in figure 1.2.
Numerous studies followed employing the same method. Among them, we mention \citet{refId0} who analyzed
the observation of AS209, \citet{Toci_2019,Toci_2020} that modelled respectively HD169142 and PDS70 and 
\citet{Ubeira_Gabellini_2019} who studied CQTau.
Last we cite \citet{dstauv} that determined the
presence of a planet in the gap of DS Tau disc estimating a mass of $3.5 \pm 1 M_{jup}$. The simulations run 
in the context of this last article were also used in this thesis for the database that will be introduced later.

Despite their power and virtual unlimited applications, numerical simulations are far to being the definitive solution
to the characterisation problem, due to the enormous amount of computational resources and time 
required to run a simulation, and the dozens of physical properties parametrizing the theoretical models which need to be 
assumed or measured with different methods.

In this thesis we want to propose a new approach based on machine learning techniques, to overcome 
the main limits of the currently used methodologies. We suggest training a neural network
in order to obtain a model with the ability to infer a set of the discs or planets features directly
analyzing the highly resoluted images of these objects. This approach would allow 
to relax some of the assumptions needed for running numerical simulations and would also require 
less computational resources.

We thus designed a dataset of synthetic observations of protoplanetary discs collecting
results of numerical simulations run in the context of other researches and kindly shared with us
for this project. We developed the storage structure, a python package and some scripts which interface 
the main functions required to handle the database. At this stage we were able to collect 24720 different images 
of protoplanetary discs, significantly exploring five parameters: the gas to dust mass ratio, the age of the disc, its inclination, the mass of
the embedded planet and the size of a 
gaussian beam used to convolve the image with the purpose of reproducing the limited resolutions
of telescopes.
We finally proved the feasibility of this new approach implementing 
a simple neural network using TensorFlow libraries with encouraging results.

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{images/hltau.jpg}
    \end{center}
    \caption{Comparison between the ALMA image of HL Tau (left) with a simulated observation (right)
    in thier continuum emission at 1.3 mm (233 GHz). Note the different colour bars.
    The simulation was run with three embedded planets with masses of 0.2, 0.27 and 0.55 $M_{Jup}$ 
    and reproduced all the major features of HL Tau observed as it can be seen from this comparison.
    Reprinted from \citet{Dipierro_2015}.}
\end{figure}

The content of this thesis is structured in seven chapters, comprehensive of this introduction and
the conclusion.

I will start, in chapter 2, introducing the subject of protoplanetary discs and planet formation which 
are the object of our study. I will describe their key properties and explain the meaning of the parameters 
which will be used to characterize the disc and its dynamic. Therefore, in the third chapter, I will review the current 
methods used for the characterisation of protoplanetary discs discussing strengths and limitations
of both analytical expressions and numerical simulations.

Machine learning, and more specifically neural networks, will 
be introduced in the following chapter explaining the principle at the core of their functioning and pondering 
their potential. I will finally propose the application of these techniques on protoplanetary discs images
reviewing a previous attempt \citep{Auddy_2020} and analyzing the feasibility, the limits and the potential of our approach.

In the last two chapters I will present the work done in this thesis. The fifth chapter describes the database we built
explaining our choices in its development and presenting the features of the version deployed with this thesis.
The python code implemented to facilitate its access and mantainance will also be discussed stressing on its future-proof design.
Perspectives on its improvement will also be provided.
The last chapter concludes the work with a proof of concept.

\chapter{Protoplanetary discs}
%Very brief introduction (1/2 sentences) about what a protoplanetary disc is,
%how they generate, where we can find them and why they are studied.

Protoplanetary discs emerge in the context of star formation. 
The process takes place in specific ``star forming'' regions filled with a mixture of gases, mainly hydrogen and helium enriched with some
heavier elements, called interstellar medium. The gravitational collapse of a denser zone gives birth to a star. Gradually the star
accrete its mass drawing, from its surroundings, gas and dust that organizes in an accretion disc whose shape is a consequence of angular momentum conservation.
At some point, after millions of years, most of the matter has accreted onto the star,
the disc temperature and thermal emission are maintained by the star irradiation while gravitational collapse becomes negligible as a source of energy (passive disc). 
It is at this stage that the accretion disc gets identified
as a protoplanetary disc due to its role as a planet formation cradle.
Their study allows the development and test of theoretical models regarding planets and their genesis.

In this chapter I am going to give an overview of the structure and observable features
of protoplanetary discs and of their relation with young embedded planets. The discussion will not be
exhaustive and neither detailed, the aim is to give
the reader a picture of these objects, and provide a basic understanding of the 
properties which I will be referring to in the next chapters.

\section{Structural properties}

\begin{figure}
    \begin{center}
    \raisebox{+0.2\height}{
        \begin{tikzpicture}
            \draw[ultra thick, color=black, domain=0.25:2, fill=gray, fill opacity=0.25] (0.25,0) -- plot (\x,{(\x*0.5)^(5/4)}) -- (2,0);
            \draw[ultra thick,color=black, domain=-2:-0.25, fill=gray, fill opacity=0.25] (-2,0) -- plot (\x,{(-\x*0.5)^(5/4)}) -- (-0.25, 0);
            \draw[ultra thick,color=black, domain=0.25:2, fill=gray, fill opacity=0.25] (0.25,0) -- plot (\x,{-(\x*0.5)^(5/4)}) -- (2,0);
            \draw[ultra thick,color=black, domain=-2:-0.25, fill=gray, fill opacity=0.25] (-2,0) -- plot (\x,{-(-\x*0.5)^(5/4)}) -- (-0.25,0);
            \draw[ultra thick,color=black] (2, 1) -- (2,-0.1);
            \draw[ultra thick,color=black] (2, -1) -- (2,-0.45);
            \draw[ultra thick,color=black] (-2, 1) -- (-2,-1);
            \draw[ thick,->, color=gray] (-2.5, -1.5) -- (-2.5, 1.5) node[left, scale=1.4] {$z$};
            \draw[ thick,color=gray, dashed] (-2.6, 0) node[left] {\small 0} -- (2.5, 0) node[below left, yshift=-1mm, xshift=3mm,  fill=white, rounded corners=2pt,inner sep=1.5pt] {\small midplane};
            \draw[ultra thick, fill=gray, fill opacity=0.25] (0,4) circle [radius=2cm];
            \draw[ultra thick, fill=white] (0,4) circle [radius=0.25cm];
            \node[star, fill=black, scale=0.6] at (0,4) {};
            \draw[ thick,color=gray] (-2.5,4) circle [radius=1mm] node[above left, scale=1.4] {$z$};
            \draw[ thick,color=gray, fill=gray] (-2.5,4) circle [radius=0.1mm];
            \draw[ thick,->, color=gray] (0,4) -- (2, 5.5) node[right, scale=1.4] {$r$};
            \draw[ thick,color=gray] (1,4) node[above right, scale=1.4] {$\phi$} arc (0:36.5:1cm);
            \draw[ thick,color=gray, dashed] (-2, 4) -- (2,4);
            \node[star, fill=black, scale=0.6] at (0,0) {};
            \draw[thin] (0, 1.5) circle [radius=0.1mm];
            \draw[thin] (0.2, 1.5) -- (0.5, 1.5);
            \draw[thin] (-0.2, 1.5) -- (-0.5, 1.5);
            %\draw[thin] (3, 1.5) circle [radius=0.1mm];
            %\draw[thin] (3, 1.3) -- (3, 1);
            %\draw[thin] (3, 1.7) -- (3, 2);
        \end{tikzpicture}}
        %\quadd
    \input{images/profile.pgf}
    \end{center}
    \caption{(a) left panel, scheme of the frame of reference used to refer to
    spatial dependencies of disc properties. (b) right panel, map of the 
    gas density in the $r-z$ plane for a flared disk ($H(r) \propto r^{5/4}$) at vertical hydrostatic equilibrium.
}
\end{figure}

\begin{comment}
Here I am going to discuss some key properties of protoplanetary discs and give
gross estimates of their typical values. I am going to discuss: 

— what discs are made of

— absolute and relative masses of gas and dust components

— dimension and distance

— temperature
\end{comment}

Most of protoplanetary discs are observed at distances of approximately 100-200 pc in the star-forming regions. 
They exhibit typically a diameter of about 100 a.u. meaning that they span approximately
1 arcsec of the sky as seen from Earth.
In the following sections I will use cylindrical coordinates 
defining the frame of reference in figure 2.1a to discuss disc properties. Only axisymmetric discs will be considered.

After their shape and location, the first question we have to address regards what discs are made of.
Two main structural constituents can be distinguished according to their physical state: gas and solids.
The solid component consists in dust and debris of various dimensions, going from the micrometers to (maybe) a few 
meters, and account for about the 1\% of the total mass. Despite being a small fraction of the disc, solid grains
are actually easier to observe and measure through their thermal emission. Measures of their mass are, however, tied to
some unknown optical properties whose estimation introduces a source of uncertainty.

Dust and solid fragments are embedded in the gaseous medium which provides the vast majority of the disc mass.
Molecular hydrogen (H$_2$) is the main constituent which is challenging to observe due to
its lack of a dipole moment. 
Therefore, measures related to less abundant molecules, such as HD or CO, are used to probe the properties of the gas component. 

The overall mass of a typical protoplanetary disc has been measured to be some Jupiter masses.
Estimates of these quantities are relevant, for example, to provide upper limits
to the masses of forming planets.

Another important property is the gas and dust temperature, closely related to sundry factors both in the disc dynamics and radiative emission.
Its value changes with the radial and vertical distance from the star going from hundreds of Kelvin to approximately 20K.
The interstellar medium instead has observable features compatible with a temperature of about 10K. Protoplanetary discs are embedded in this medium 
that hence constitutes the background of observations and makes it difficult to reveal colder parts.

\section{Disc dynamics}

\begin{comment}
Here I am going to explain how the dynamic of gas and dust is modelled.

I am going to provide the equations describing the vertical structure, explain
the meaning of the aspect ratio and the viscous forces at play.

I will also explain the model describing the interaction
between the gas and solid components (Epstein force, stokes number).

Finally, I am going to cite other forces and effects which play a role in disc dynamics,
such as magnetorotational instability, turbulence, winds, photoevaporation, ...
\end{comment}

\begin{comment}
Now that the discs composition and some of their properties have been established, I am going 
to investigate the physics governing each component and explain how they are shaped by it.
\end{comment}

The gas and dust components just discussed, 
evolve and settle in characteristic structures, which I am now going to discuss.

The disc evolution can be properly described by the laws of fluidodynamics in the gravitational potential of the central star.
%add NS equations (?)
Some assumptions need to be made in order to acquire a predictive model of practical use.
The first one is called ``thin-disc approximation'' which consists in assuming that the radial distance is larger than 
the vertical typical length scale $H$, thus requiring $H/R \ll 1$. This quantity, called aspect-ratio, has been measured, 
showing values in the range $10^{-3}-10^{-1}$ which justify the assumption. The thin disk approximation allows the study of disc properties integrating
the equations along the vertical direction.
The second simplification that is usually made consists in neglecting self-gravity.
The stability condition of the disc against self-gravity can be written \citep[p. 40]{book_planet_form} as
\begin{equation}
    \frac{M_{disc}}{M_{star}} \lesssim  \frac{H}{R}
\end{equation}
which is well satisfied in the late epochs when protoplanetary discs are studied.

Keeping in mind these assumptions, I am going to further explore how fluid dynamics can be applied to model the disc 
structure and its internal motions.

The models described below are simplifications of the complex dynamics of protoplanetary discs. 
There are many secondary effects which will not be discussed but
contributes to the proper description of discs evolution such as magnetorotational instability, photoevaporation and 
turbulences or vortices generated by the fluid motion in the turbulent regime. All 
this effects must be taken into account to obtain a complete description of the disc dynamics and evolution.

\subsection{Gas}

The gas component is modelled as an ideal fluid characterised
by the equation of state
\begin{equation}
    P = \frac{k_BT}{m_p\mu}\cdot \rho \equiv c_s^2 \cdot \rho
\end{equation}
where $\mu$ represents the mass of a single gas molecule expressed in masses of the proton ($m_p$).
Most of the solutions discussed in this section are obtained starting from the momentum equation
for an inviscid fluid
\begin{equation}
    \pdv{\va{v}}{t} + (\va{v}\cdot \va\nabla)\va{v} = -\frac{1}{\rho}\va  \nabla P - \va \nabla \Phi
\end{equation}
where $\va v$ is the velocity, $\rho$ the density, $P$ the pressure, and $\Phi$ the gravitational potential.

Investigating the gas component is sufficient to predict most of the macroscopical features of the discs structure due to its
predominance over the solid elements. 

The vertical structure of the gas can be described by the hydrostatic solution
of equation 2.3 whose right term is thus set to zero giving the balancing condition of the vertical
accelerations due to gravity and pressure \citep[pp. 38-41]{book_planet_form}. Assuming a vertically isothermal disc to write the pressure as $P=\rho c_s^2$ (equation 2.2)
and exploiting the thin 
disc assumption for the projection of the gravitational
 force along the vertical direction,
the balancing condition gives
\begin{equation}
    c_s^2\dv{\rho}{z}= -\Omega_K^2z\cdot \rho.
\end{equation}
The solution of this equation provides the vertical density profile
\begin{equation}
    \rho(z, r) = \rho_0(r)\exp(-\frac{\Omega_K^2z^2}{2c_s^2}) \equiv \rho_0(r)\exp(-\frac{z^2}{2H^2})
\end{equation}
which also offers a quantitative definition of the previously introduced length scale
$H$: it is identified as the 
standard deviation of the Gaussian vertical density profile.
The following relation is also provided
\begin{equation}
    H = \frac{c_s}{\Omega_K}.
\end{equation}
In all these equations $\Omega_K$ is the Keplerian 
angular velocity while $c_s$ is the sound speed defined as $c_s^2 \equiv \dv{P}{\rho}$, which is equal to 
\begin{equation}
    c_s = \sqrt{\frac{k_BT}{\mu m_p}}
\end{equation}
for an 
ideal fluid described by the equation of state 2.2.

The height scale $H$ is thus proportional to $T^{1/2}$. Its radial dependence
can be studied computing the radiative equilibrium of the disc, in order to obtain the 
radial temperature profile which is related to $H(r)$ through equations 2.4 and 2.5.
Assuming a passive and flared disc, calculations usually lead to $T(r) \propto r^{-1/2}$ 
meaning $H(r) \propto r^{5/4}$ which is consistent with the ``flared assumption'', requiring the index of this last
power law to be greater than 1.
Figure 2.1b shows the vertical density distribution in a disc characterized by the just mentioned $H(r)$ radial profile
and equation 2.3.

The overall dynamics of the gas is driven by the star accretion process. In order to explain this motion, some mechanisms of energy dissipation and angular momentum transport
are needed. Viscosity plays a central role in this context. Detailed calculations show that shear viscosity, 
as modelled in an ideal gas and caused by molecular collisions is too weak to account for these processes. 
A proper model can be achieved assuming a highly turbulent regime that can produce a ``turbulent viscosity'' through the mixing of fluid elements
at neighboring radii. The new parameter $\alpha$
called ``Shakura-Sunyaev viscosity'' \citep{ssviscosity} which gathers all the ignorance on this process is introduced through the following reasoning:
in the case of an isotropic and newtonian fluid, dynamic viscosity $\mu$ is introduced as a scalar constant of 
proportionality between the off-diagonal component of the stress tensor $\sigma_{ij}$ and the strain rate tensor $e_{ij}$ which is the symmetrised gradient of velocity \citep{fluid}
\begin{equation}
    \sigma_{ij} = 2 \cdot \mu \cdot e_{ij} = \mu \cdot \left( \pdv{u_i}{x_j} + \pdv{u_j}{x_i} \right), \hspace{5pt} i \neq j
\end{equation}
where the stress tensor $\sigma_{ij}$ is the force in the direction $i$ acting on a surface whose
normal is in direction $j$.
This additional effect to the fluid dynamics lead to the addition of a new term to the momentum equation (equation 2.3):
\begin{equation}
    \pdv{\va{v}}{t} + (\va{v}\cdot \va\nabla)\va{v} = -\frac{1}{\rho}\va  \nabla P - \va \nabla \Phi + \frac{\mu}{\rho}
    \left[ \nabla^2\va u + \frac 1 3 \va \nabla \left( \va \nabla \cdot \va u\right) \right]
\end{equation}
From the dynamic viscosity $\mu$ a new constant, called kinematic viscosity $\nu$, is introduced as $\nu=\mu/\rho$.
In an ideal gas the kinematic viscosity coefficient can be shown to satisfy $\nu = \frac{1}{3}c_s\lambda$,
with $\lambda$ indicating the mean free path of particles in the fluid.
To describe the turbulent viscosity we proceed by analogy assuming $\nu_T \sim u_T \lambda_T$. In this equation $u_T$ indicates a typical velocity of the turbulent regime
which should 
satisfy $u_T \lesssim c_s$ because upper velocities would lead to shocks thermalizing the turbulent motion.
The $\lambda_T$ factor 
represents the typical length scale which, assuming isotropic turbulence, can not be greater than $H$, the disc height scale.
Therefore, we obtain the equation
\begin{equation}
    \nu = \alpha c_s H
\end{equation}
which provides a method to estimate the turbulent viscosity introducing the $\alpha$ coefficient that, following the arguments above,
must take values less than 1.

The orbital velocity of the gas is determined by the gravitational attraction of the 
star inducing a keplerian orbit which is slightly modified by pressure forces.
This effect can be determined starting from the momentum equation 2.3.
Assuming a stationary ($\pdv{\va v}{t}= 0$) axisymmetric flow and the sole gravitational potential of the star,
its radial component becomes
\begin{equation}
    \frac{v_{\Phi, gas}^2}{r} = \frac{GM_{star}}{r^2} + \frac 1 \rho \dv{P}{r}
\end{equation}
analogous to the keplerian expression for the orbital velocity with the additional
term that describes the pressure support.
Since near the disc midplane the pressure decreases outward, this term 
is negative meaning that the gas azimuthal velocity is slightly less than the keplerian velocity 
of a particle orbiting at the same radius.

Describing the pressure radial dependence with a power las as 
$P = P_0 \left(\frac{r}{r_0}\right)^{-\eta}$ with the introduction of 
the positive index $\eta$, it can be shown that 
\begin{equation}
    v_{\phi, gas} = v_K \left[ 1 - \eta \left(\frac{H}{r} \right)^2 \right] ^{1/2}
\end{equation}
exposing also the role of the aspect ratio
in the azimuthal motion of the gas.

These corrections to the keplerian velocities are negligible when considering the motion of the gas alone.
For example, for a disc with a constant aspect ratio $H/r = 0.05$ and the index $\eta = 3$, obtained from 
a gas surface density $\Sigma(r) \propto r^{-1}$, we obtain
\begin{equation}
    v_{\phi,gas} \simeq 0.996v_K.
\end{equation}
However, these differences are important for the evolution
of the solid grains due to mechanisms that will be explained in the next section.
\subsection{Dust}

The dust component is modelled as a pressureless fluid with grains of different dimensions coupled to the gas medium. 
The strength of the coupling is expressed by the Stokes number $St$ characterizing the behaviour of
the dust grains suspended in the gas.
It is defined as the ratio of the friction timescale of the dust particle to the characteristich time of the flow.
In protoplanetary discs, the main motion of the gas component is the (approximately)
keplerian rotation around the central star, the Stokes number can thus be defined, in this case,
as 
\begin{equation}
    St = t_{stop} \cdot \Omega_K
\end{equation}
using the keplerian angular velocity $\Omega_K = \sqrt{\frac{GM_{star}}{r^3}}$.
The friction timescale $t_{stop}$ measures the time in which the drag modifies the relative velocity of the 
gas particle considered significantly. It can be computed from the particle mass ($m$), 
its relative velocity with respect to the gas ($v$)
and the drag force module:
\begin{equation}
    t_{stop} = \frac{m\cdot v}{|F_D|}.
\end{equation}

Two drag forces come into play depending on the grain size $s$ \citep[pp. 110,111]{book_planet_form}. 
If $s \lesssim \lambda$, with $\lambda$ indicating the mean free path of gas molecules, 
the drag force is called Epstein drag. 
In this regime, which is usually the most relevant for most particle sizes, 
the drag is caused by gas molecules which collide against the front and back sides of the grain with different 
frequencies due to its motion relative to the gas medium.
From the equations 2.13 and 2.14, computing the module of the Epstein force, we can obtain the expression 
for the Stokes number in the Epstein regime 
\begin{equation}
    St = \frac{\pi}{2}\frac{s\rho_{gr}}{\Sigma_g}.
\end{equation}
It thus depends on the size ($s$)
and density ($\rho_{gr}$) of the particles that experience the drag
and to the gas distribution through its
surface density ($\Sigma_g$) assuming 
$\Sigma_g = \sqrt{2\pi}H\cdot \rho_g(z=0)$ which is the result of the integration
along the vertical direction $z$ of equation 2.5.

Once sizes much larger than the molecular 
mean free path are reached, the solid grains begin to experience a force of different nature called Stokes drag.
In this condition the Stokes number is given by different
equations depending on the laminar or turbulent regime
of the flow.

Strongly coupled grains, which thus exhibit $St \ll 1$ and are usually of small dimensions,
are essentially stationary in the gas and thus dragged 
at its subkeplerian velocity (see section 2.2.1). In addition to that,
dust particles do not experience the same pressure force of the gas component.
As a consequence, their centrifugal force is not sufficient to balance the gravitational attraction 
that hence makes them spiral inward towards a pressure maximum where the gas orbital velocity, given by equation 
2.10 becomes keplerian due to the vanishing of the pressure term.

Large rocks poorly coupled to the gas, and thus with $St \gg 1$, exhibit the same radial drift. 
In this case the solids start with a keplerian orbital motion, the high relative velocity with respect to the gas
medium makes them esperience large drag forces in the opposite direction that remove angular momentum causing 
the inward motion. In both cases solid grains are thus trapped in local pressure maxima.

\section{Observations}

\begin{comment}
Here I am going to explain how discs are observed presenting the different observational primers 
for the gas and the dust component. I am going to present some links between structural and observational 
properties (such as $\lambda \sim s/2\pi$).

I am also going to explain which is the best image resolution currently achievable.
\end{comment}

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{images/2.2.tracers.comparison/apjaba1e1f9_hr.jpg}
    \end{center}
    \caption{Comparison between observations of the RU Lup disc through the three main tracers.
    Left: map of $^{12}$CO emmission. Center: map of $\lambda=1.25$mm emission from the thermal continuum.
    Right: map of scattered light. Different morphologies can be observed depending on the tracer.
    Reprinted from \citet{img_2.2}.}
\end{figure}


Direct observations of protoplanetary discs are of crucial importance for the detection and study of their morphology.
Properties of substructures, such as their shape and dimension, had, in fact, 
been proved to reveal key features of the disc itself and
of the objects that drive their formation. I am going to discuss how they are detected.

Due to their distance we can only look at their electromagnetic emission
and exploit the natural diversity of structures and properties. 
Three sources of light can be identified \citep{disc_rev}. Examples of their
observations can be seen in figure 2.2.

The first one is the radiation, usually of micrometric wavelength, emitted by the host star and scattered by small (micron-sized) 
dust grains.
This tracer is especially sensitive to the vertical structure 
of the disc allowing measures of the height and of the dust distribution along this direction.
This infrared emission is usually observed by either ground based high contrast imagers, such as SPHERE or the Very Large Telescope (VLT), 
or by space observatories, such as the Hubble Space Telescope (HST) and, in the near future, the James Webb Space Telescope (JWST).

The second tracer is the thermal continuum emission of solids, typically at millimetric wavelengths,
emitted in the optically thin regime which implies direct proportionality between the intensity of the radiation observed and the
mass density of the dust. 
Images of the disc thermal emission are, thus, of specific interest in the investigation of larger (mm-sized) solid grains.
This emission is typically detected by the Very Large Array (VLA) or the Atacama Large Millimiter Array (ALMA) which are both arrays
of respectively 27 and 66 radio telescopes.

On the contrary, the gas component eludes direct observations due to the nature of $H_2$ the dominant molecule.
This part of the disc can be detected thanks to spectral line emission of less abundant molecules,
the most common being CO in its various isotopologues,
that constitute the third category of observational primers. Measures made from these probes suffer from
the uncertainties in the molecular abundance of the revealed gases along with other problems
which make gas observation complex and uncertain.

In all of these cases the range of wavelengths probed is approximately $\SI{1}{\micro\m} - \SI{10}{\mm}$, hence going from infrared to radio waves.
As anticipated, protoplanetary discs have a diameter of about 100 a.u. at a typical distance of 100/150 pc. Therefore, telescopes need to acquire an angular
resolution under the arc second which is done using interferometric techniques.

The images with the best resolution currently obtained are those of the DSHARP (Disk Substructures at High Angular Resolution Project) survey \citep{dsharp}, $\sim 0.035$ arcsec, designed to examine properties
of small-scale substructures and inquire how they relate to the planet formation process. In figure 2.3, some of the images obtained in the context of this project
can be appreciated.

\section{Planet formation}

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{images/alma_pictures.jpg}
    \end{center}
    \caption{Examples of gaps and double gaps. 
    Observations of the 1.25 mm dust continuum emission of some discs obtained from DSHARP.
    Beam sizes and 10 au scalebars are shown in the lower left and right corners of each panel, respectively.
    Image adapted from \citet{Andrews_2018}.}
\end{figure}

\begin{comment}
Here I am going to discuss how planets are formed within these discs. 
I will explain: 

    — how they accrete their mass

    — the forces they experience and thus the radial drift

    — the substructures they form in the disc

    — typical mass values and their relation with substructures (qualitatively) 
\end{comment}

\begin{comment}
-introduction on the importance of planet formation and how ppd are the natural birthplace of planets
-how we go from debris to massive planets, mechanisms and the motion of the planet inside the disc
-substructures ad how they are related to the planet mass
\end{comment}

In 2006 the International Astronomical Union defined a planet as a celestial body in orbit around the sun with enough mass 
to reach a hydrostatic equilibrium (nearly round) shape and which has cleared the neighbourhood around its orbit. Outside the solar system 
the term planet indicates large bodies, in orbit around a star,
with a mass below the limit for thermonuclear fusion of deuterium, 
currently calculated to be 13 Jupiter masses for objects of solar metallicity. The lower mass threshold should instead be the same of that considered
for the solar system.

The study of extrasolar planets, often referred to as exoplanets, is of primary importance to make statistical studies of theories 
involving planets and their formation. Protoplanetary discs are the planets' birthplace. 
Different mechanisms of planet formation have been proposed and some parts of the theories are not completely
understood yet. 

The core accretion process \citep[pp. 25-26]{veronesi_phdthesis} is believed to be the origin of planets from the dust component of the disc. 
The process starts from the small dust grains which are well coupled to the gas.
The coupling makes them acquire low relative velocities that result in gentle collisions forming aggregates called, when a significant mass is reached, planetesimals.
While gradually accreting their mass, this aggregates settle towards the mid-plane and start to decouple from the gas experiencing a radial force 
which makes them spiral inwards toward the global maximum in the gas pressure. This motion is called radial drift and it is usually more effective for grains of sizes in the range 
1-10 mm.
The accretion of planetesimals continues, with an increasing relevance of gravity in the process,
until, in a few million of years, the local dust population is depleted by radial migration or the collisions become destructive.

This process profoundly influences the morphology of the disc leaving us ways to detect and characterize newborn planets. 
Gaps, annular regions of dust or gas depletion, are the pivotal substructure generated from the gravitational interaction of the disc with young planets.
Links between their shape and planets features were found, which will be discussed in detail in the next chapter:
a measure of their width allows, for example, an indirect estimation of the planet mass.

\section{Gaps and substructures}

\begin{comment}
Here I am going to:

— explain how planets are not the only thing that generates gaps

— present the radial profile of a gap in dust and gas densities

— explain the differences between gap structures in gas and dust 

— explain how depth and width are defined
\end{comment}

A wide variety of substructures has been identified in the highly resoluted images
of protoplanetary discs in their scattered light and millimetric emission. Some examples are shown 
in figure 2.4 that depicts the four broad categories under which are grouped the various morphological types:
from left to right, cavities, gaps, arcs and spirals.

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{images/subs.png}
    \end{center}
    \caption{Examples of substructures in the dust continumm emission and in scattered light observations.
    From left to right each column contains examples of, in this order: ring/cavity substructures, gaps, arcs and spirals.
    Beam sizes and 10 au scalebars are shown in the lower left and right corners of each panel, respectively.
    The discs depicted are, from left to right: (top) Sz 91 \citep{sub1}, Elias 24 \citep{Andrews_2018}, SAO 206462 \citep{sub3},
    Elias 27 \citep{sub4}; (bottom) RX J1604.3-2130 \citep{subs5}, HD 163296 \citep{subs6}, V1247 Ori \citep{subs7}, MWC 758 \citep{subs8}.}
\end{figure}

Gaps are going to be the object of application for the investigative approach proposed in this thesis due
to their role in the detection of young planets.
For this reason I am going to throw some light on their properties.

They are one of the most common substructure observed in images of protoplanetary discs, and
planets are often the cause of their formation \citep{gap_opening1, gap_opening2, gap_opening3}. However, other mechanisms were recognized to generate gaps
including magnetorotational instability turbulence \citep{MRI}, gravitational instabilities \citep{GI}, condensation of molecular species along different snowlines \citep{cond},
large-scale vortices \citep{vortix} and self-induced dust pileups \citep{dust_pup}. The effective presence of a planet must hence be cautiously investigated and confirmed 
before inferring its features from the gaps properties. 

The ability of a planet to carve a gap in the disc depends upon several elements. 
\citet{Dipierro_2017} have derived an analytical criterion relating the minimum mass required for a planet
to open a gap to the key properties of the dust: the Stokes number, the aspect ratio and the gas to dust mass ratio.
They showed that the critical mass required to open a gap in the dust is lower for large Stokes numbers
and large aspect ratio.
The order of magnitude, for typical discs, of the planet's mass threshold for gap opening was computed in $10^{-3}M_{Jup}$
which is in the order of the earth mass. They also determined that low mass planet could carve gaps only if the
Stokes number was above a critical value of order unity.

Figure 2.4 presents the radial profile of the gas and dust densities at a gap. Taking into exam 
the graph relative to the dust density, I am going to highlight some recurrent features:
at the core of the depletion zone, we can observe two minimums separated by a local maximum which usually coincides with the planet orbit.
Sometimes this maximum reaches values comparable with the density of the unperturbed disc, a substructure that can be identified as a double gap. 
The edges also presents local maximums. Gaps are usually wider and deeper in the dust rather than in the gas component where they could even
fail to form.

\begin{figure}
    \begin{center}
        \input{images/rad_profiles.pgf}
    \end{center}
    \caption{Azimuthally averaged radial profiles of the surface density of the gas component (left panel)
    and of the dust (right panel). The graphs were computed from a snapshot of one of the hydrodynamical
    simulations used by \citet{dstauv} to reproduce the observed image of DS Tau.
    The disc was simulated starting with an embedded planet at a radial distance of $R_p = 34.5$ a.u. with a mass of 3 $M_{Jup}$ and a das to dust ratio of 100.
    The snapshot was taken after 250 complete orbits of the planet around the central star.
    }
\end{figure}

Width and depths are the crucial properties of gaps used to investigate planet properties. Despite their importance there are not agreed on definitions.
For the work done in this thesis they are not needed as the neural network model we aim to build would work directly with disc images.
However, currently used methods, which will be discussed in the next chapter, include analytical tools which explicitly require the width
value that thus needs to be defined.
There are different definitions used to give a value to the gap width: \citet{Long_2018}, for example, defines it as the full width at half depth where 
the depth is computed as the difference between the density (or the emission intensity) at the minimum and at the outer peak.
Due to the empirical nature of the analytical formulae that will be discussed, a change in the chosen definition requires to simply recompute
some coefficients.                                        

The depth can also be defined as the minimum of the density profile normalized with the initial unperturbed value. Sometimes the intensity of the emitted light is used in place of the
density to obtain a definition which depends upon directly measurable properties.

\chapter{State-of-the-art characterisation methodologies}

Protoplanetary discs are interesting objects both in their own dynamic and
in the context of planet formation.
Many models were developed to describe and explain the disc motion and the origin of substructures and
some questions are still open. However, these problems will not be the object of this chapter, 
here I am going to focus on the methods developed to interpret the
images of protoplanetary discs, obtained through direct observations, in order to infer their properties.

\section{Addressed questions}


The only property of protoplanetary discs that can be directly measured
is the intensity of the light emitted with the different mechanisms previously discussed.
The other features and parameters which characterize them and the embedded astronomical objects
must be extrapolated from these measures. 
Tools and methods to achieve this purpose are central branches of research. In this chapter I am going to discuss
the techniques so far developed and used, comparing their strengths and limitations in order to present the context
in which the use of machine learning techniques will be proposed.


Features that can be extrapolated from the data fall into three categories: optical properties, hydrodynamical properties
and properties of the central star and orbiting planets.
Determining them from observations is crucial
to properly apply the developed models, test theories and make predictions on the disc behaviour.
These features are not all unrelated, in some cases equations obtained from the theoretical
models can link some of them while for other ones some empirical relationships can be found.
This intricate net of dependencies come useful in the characterisation of the disc and provides 
ways to check the results.

The following sections focus on the analysis of the
techniques used to infer the mass of gap opening planets, which is
the problem chosen in this thesis to investigate the machine learning approach.
However, each method presented must be considered an example for the respective category. 
In fact, with proper adjustments, the same approaches are
applied to infer other properties of the planet or of the disc.

\section{Analytical formulae}

\begin{comment}
    In this section I am going to explain that many disc features can be analytically 
    linked using simple linear or power laws.  
\end{comment}

Observations provide direct measures of the light tracers intensity, both spatially resolved and as
the total disc luminosity, often expressed separately for wavelength or source.
In addition to these data, from the images obtained, substructures' shapes and
dimensions can be observed and computed.
A first attempt to interpret the data consists in researching empirical patterns in their relations with some
interesting features of the disc. These links are often expressed as power laws or polynomial equations whose parameters are
determined through a regression of available data.
For example, the scaling relation between the disc luminosity at millimetric wavelengths and the star mass $L_{mm} \propto M_*^{1.7\pm0.3}$
is exhibited by discs with mean ages $\lesssim 3$ Myr \citep[p. 13]{disc_rev}.
In addition to empirical formulae, theoretical models, supported by assumptions on
the optical properties of the gas and dust in the disc, offer some equations useful
in the determination of structural properties.

\subsection{Planet mass and gap width}

\begin{comment}
I am going to focus on the link between planet's mass and gap width,
providing the `Lodato' and `Kanagawa' models.
\end{comment}

In the context of planet formation special attention is reserved to the study of gaps. 
We have seen that a possible explanation regarding their origin can lie in the interaction with
forming planets. If we consider axisymmetric gaps, their width is the main discriminative property and
has been linked to the mass of the planet responsible for their origin.
For example, a direct proportionality between the gap width and the planet's Hill radius has been suggested \citep{Lodato_2019}, where
$
    R_H = (\frac{M_P}{3M_*})^{1/3}R_0
$
is the Hill radius, $M_P$ the planet mass, $M_*$ the stellar mass and $R_0$ the planet orbital radius.
This leads to 
\begin{equation}
    M_P = (\frac{w_d}{k\cdot R_0})^3\cdot 3M_*
\end{equation}
providing a relation between the planet mass and the gap width $w_d$ measured in the dust component.
The coefficient $k$ is introduced to model the proportionality and must be regressed from the data.

A different approach which had been proposed, \citet{kanagawa} suggests the following relation
\begin{equation}
    M_P = 0.0021 \cdot \left( \frac{w_g}{R_0}\right)^2
    \cdot \left(\frac{h_0}{0.05}\right)^{\frac 32}
    \cdot \left(\frac{\alpha}{10^{-3}}\right)^{\frac 12}
    \cdot M_*
\end{equation}
which links the planet mass to the gap width measured in the gas component $w_g$.
This last equation includes more features related to the hydrodynamic of the disc which could, in principle,
influence the development of different gap structures: the aspect ratio at the planet position
$h_0$ and the $\alpha$-viscosity. Accounting for these additional features should result in more accurate predictions
across protoplanetary discs in which they differ significantly. It has indeed been shown
\citep{Dipierro_2017} that some physical properties of the disc 
 influence the gap opening mechanism varying the ability of planets to carve gaps
 in the disc with differences depending on their mass. In the same paper they also 
 determined an analytical relation linking the distance of the planet to the outer edge of the gap
 to the gas to dust density ratio, the Stokes number, an index related to the pressure profile and the aspect ratio. 
 However, measuring these properties, such as the $\alpha$-viscosity or the aspect-ratio,
is not an easy task, and it introduces a source of error.
Another downside of the \citet{kanagawa} equation is that it uses the gap width $w_g$ measured in the gas component 
whose density map is more difficult to observe and resolve.

\subsection{Strengths and limitations}

Analytical formulae provide a quick method to determine features, such as the planet mass, with 
very low computation. Furthermore, an analytical expression highlights relationships between variables
which could be interpreted through theoretical arguments in order to achieve a better understanding 
of the underlying physics.

On the other end, analytical expressions, especially the empirical ones, suffer in accuracy due to 
the extreme simplification of the functional forms used, and highlight relationships between a restricted subset of
the variables which characterize the disc, potentially hiding the role of other ones.
Additionally, in order to obtain the constants that characterize these formulae we need a set of
data with known values of every variable involved. This requires having other methods to obtain this information
or, in alternative, resort to numerical simulations which start from known values of the parameters and produce
data analogous to the observations of real discs. In this case also the limitations of the numerical approach must be taken 
into account.

\begin{comment}
Here I am going to discuss the strengths and limitations of the analytical approach.
From this section it should be clear why numerical simulations are preferred.
\end{comment}

\section{Numerical approach}

\begin{comment}
In this section I am going to explain how disc features can be inferred 
from simulations of the entire disk. I am going to present the possibilities
in the choices of the simulating software.
I am then going to focus on a specific choice and discuss the main steps of the 
simulation workflow.

This section plays a double purpose: it presents the current approach for
the study of protoplanetary discs and explains how the images used to build the 
database were generated.
\end{comment}

Computer simulations apply the models developed to describe the physics of protoplanetary discs 
and return predictions in the data space, which can then be directly compared with observations.
The remarkable computational power that we can nowadays achieve allows the application of
more complex models which reduce the approximations and are thus able to obtain more 
accurate results.

This approach can be used to infer discs or planets features, exploring some values through different simulations
whose output is then compared with the actual data to decide the best fit.
The simulation workflow (Figure 3.1) consists in three steps. 
First, starting from an appropriately chosen initial condition,
we compute the fluidodynamical evolution of the disc obtaining its configuration
at a given time. The second step is aimed at determining the radiative emission and trace it 
to simulate an observation from a specific distance and relative orientation.
In order to predict the distribution and intensity of the light emitted, this step begins computing
how the star radiates its energy and producing a map of the disc temperature. These processes involving the
transfer of energy in the form of electromagnetic radiation are grouped under the term ``radiative transfer''.
Finally, the last step has the purpose to reproduce the noise and limited resolution of the instruments 
used to collect the real data.

\begin{figure}
    \begin{center}
        \begin{tikzpicture}
            

            %models
            \tikzstyle{block} = [rectangle, rounded corners, text width=4cm, minimum height=3cm,text centered, draw=black, fill=gray!30]
            \tikzstyle{io} = [rectangle, rounded corners,text width=4cm, minimum height=1cm,text centered, align=left, font=\scriptsize, draw=black]
            \tikzstyle{block2} = [rectangle, rounded corners, text width=3cm, minimum height=6cm,text justified, draw=black, inner sep =0.5cm]
            
            
            %main rectangles
            \node (phantom) [block] {\large \bf Hydrodynamical simulation \\ \normalfont ... \\ \small Software choice: PHANTOM};
            \node (mcfost) [block, right of=phantom, xshift=4cm]
            {\large \bf Radiative \\transfer \\ \normalfont ... \\ \small Software choice: \\ MCFOST};
            \node (pymcfost) [block, right of=mcfost, xshift=4cm] {\large \bf Observational \\limits \\ \normalfont ... \\ \small Software choice: \\ PYMCFOST};

            %arrows
            \draw [->] (phantom) -- (mcfost);
            \draw [->] (mcfost) -- (pymcfost);

            \begin{scope}[on background layer]
            \node (iph) [block2, below of=phantom, yshift=-2cm] {
                \scriptsize
                \\ \\ \\ \\
                \begin{itemize}
                    \item requires hydrodynamical and structural parameters of the disc \\
                    \item computes the dust and gas distribution at a given time \\
                    \item produces dump files which can be directly read by MCFOST
                \end{itemize}
            
            };


            \node (iph2) [block2, below of=mcfost, yshift=-2cm] {
                \scriptsize
                 \\ \\ \\ \\
                \begin{itemize}
                    \item requires optical properties and relative position of the disc \\
                    \item first, compute the disc temperature resulting from the star irradiation \\
                    \item then, produces an intensity map of the emitted light at a specific wavelength
                \end{itemize}
            
            };

            \node (iph2) [block2, below of=pymcfost, yshift=-2cm] {
                \scriptsize
                 \\ \\ \\ \\
                \begin{itemize}
                    \item requires the parameters of the instruments used for the real observations \\
                    \item reproduce noise and errors which afflict observations \\
                    \item pymcfost was only used to simulate the telescopes limited resolution
                \end{itemize}
            
            };

            \end{scope}
            \begin{comment}
            \node (iph2) [io, below of=iph, yshift=-0.5cm] 
            {
                $    \rightarrow
                \begin{cases}
                 *\_00001 \\
                 *\_00002 \\
                 ... \\
                \end{cases}$
            };
            \coordinate [right of=mcfost, yshift=1.3cm, xshift=2cm] (fn1) ;
            \coordinate  [right of=iph2, xshift=1cm] (fn2);
            
            
            %\node (imcf) [io, right of=mcfost, xshift=2.6cm, yshift=0.7cm]{ $\leftarrow$ *.para};
            %\node (uuoutmcf) [io, below of=imcf, xshift=1.5cm, yshift=0.3cm]{ $\rightleftarrows$ data\_th/Temperature.fits.gz};
            %\node (outmcf) [io, below of=uuoutmcf, xshift=-0.55cm, yshift=0.3cm]{ $\rightarrow$ data\_***/RT.fits.gz};
            
            %\coordinate [right of=pymcfost, yshift=1.3cm, xshift=2cm] (fn3) ;
            %\coordinate  [right of=outmcf, xshift=1cm] (fn4);
            %\draw[->] (outmcf) -- (fn4) |- (fn3);
            %\node (ipymc) [io, below of=fn3, xshift=1.28cm, yshift=0.1cm] {$\leftarrow$ bpa, bmaj, bmin};
            \end{comment}
            \end{tikzpicture}
    \end{center}
    \caption{Typical workflow of a numerical simulation
     aimed at reproducing a synthetic image of a protoplanetary disc.}
\end{figure}

I am now going to give some insights for each of these steps. The images collected in the database for machine learning
applications that we designed, were obtained through simulations of this kind. Among the possible choices
of software available for each step, I am going to focus on the ones used to obtain the synthetic images
in this database.

\subsection{Hydrodynamical simulations}

\begin{comment}
Here I am going to give some background about phantom and the type of 
data it generates.
\end{comment}

Hydrodynamical simulations solve fluid dynamics equations to predict the evolution of the gas and dust 
modelled as fluids with the specific features discussed in section 2.2.
To run the models a starting configuration must be provided which is usually generated from given masses 
of each component and specifying, assuming axisymmetric discs, the density and sound-speed radial profiles modelled
with power laws such as:
\begin{equation}
    \Sigma(r) = \Sigma(R_{ref})\cdot\left(\frac{r}{R_{ref}}\right)^{-p}
\end{equation}
and
\begin{equation}
    c_s(r) = c_s(R_{ref})\cdot\left(\frac{r}{R_{ref}}\right)^{-q}
\end{equation}
where $R_{ref}$ is a reference radius. The density and sound-speed values at this radius, in addition to the indexes $p$ and $q$
are the input constants that characterize the initial state.
Other variables, such as the $\alpha$-viscosity, the planet and stellar masses and the orbital radius, are also provided as input parameters.

One of the available programs for this step is \lstinline{PHANTOM} \citep{phantom}, a smoothed particle hydrodynamics (SPH) code 
specifically designed for astrophysical applications and which is the one used to produce
the starting data for the synthetic images collected in the database that was developed in this thesis.
If properly compiled, \lstinline{PHANTOM} is able to run a simulation of a protoplanetary disc with a planet orbiting around the central star. 
The simulation starts from an initial condition described in the input files and produces, after each given number of completed planet orbits, a dump file
containing the spatial distribution of the gas and dust simulated particles, in addition to their velocity. 
All the information needed to restart the simulation is also stored inside these files.

\subsection{Radiative transfer}

\begin{comment}
Here I am going to discuss the software used for radiative transfer: MCFOST.
I will explain why this step is performed and the meaning of the results obtained.
\end{comment}

The next stage in the simulation workflow is the radiative transfer computation. For this
step we chose to use \lstinline{MCFOST} \citep{mcfost1,mcfost2}, a code designed for this purpose and based on Monte Carlo methods.
This program provides the possibility to run a simulation for gas and dust grains in the specific 
spatial configurations stored in a \lstinline{PHANTOM} dump file.

The aim is to obtain a synthetic observation of specific light tracers as seen from given angles and distance.
To produce the images used in this thesis we are only interested in probing the dust component through the thermally
emitted radiation at a fixed wavelength.
First of all, the radiative transfer of energy from the star to the disc elements is calculated, generating a map of the disc temperature.
At this point the thermal emission of the dust is computed and ray traced to obtain the resulting image.

The model run by \lstinline{MCFOST} depends upon optical properties, such as the optical length and the opacity, which 
are new parameters characterizing the results of the entire simulation process.

The output obtained at this stage is a 1024x1024 pixels image in the
\lstinline{FITS} (Flexible Image Transport System) format.
The header of this file stores some input parameters: 
the wavelength of the light in the image, the inclination and position angle of
the disc, its distance and the units of the data stored.

\subsection{Generation of synthetic images}

\begin{comment}
Here I am going to discuss the different methods that can be used to simulate the limitations 
of observing instruments. I will further explore some key features of pymcfost.
\end{comment}

\begin{figure}
    \begin{center}
        \scalebox{0.45}{\input{images/fig3.2.pgf}}
    \end{center}
    \caption{Comparison of a real disc observation (DS Tau disc), in the 1.3 mm dust continuum emission,
    with synthetic images computed with the purpose of reproducing the real one.
    Left: image obtained as a result of a radiative transfer simulation through \lstinline{MCFOST}.
    Center: the same image at its left processed with pymcfost to reproduce the limited
    resolution. Right: the real observation. Images adapted from \citet{dstauv,Long_2018}.}
\end{figure}

The real observations of protoplanetary discs are, being experimental data, susceptible 
to statistical and systematical errors and limited by the resolution of the instruments.
The last step of the simulation process consists in reproducing these limits in order
to obtain images as close as possible to those that we would obtain from a real observation.
Among the different types of errors, the tools available reproduce the
statistical fluctuations while, systematical errors are usually not introduced as they should be 
looked for and removed or corrected from the original measurements.
This step is needed to allow a better comparison with the real images or to train machine learning 
models which should then be able to work with the real observations.

There are different tools which could be used to achieve this purpose.
The \lstinline{CASA} (Common Astronomy Software Applications) package \citep{casa} is the most complete choice 
which is specifically designed to reproduce images as they would be captured by ALMA, reproducing the noise in the data,
the limited resolution of the instruments and taking into account the spatial configuration of the radio telescopes
and other settings which could influence the observations.

The images in the database assembled in this thesis were processed using \lstinline{pymcfost}, a less powerful 
python package designed to work with the files produced by \lstinline{MCFOST}.
I used this software only to reproduce the telescope limited resolution while statistical errors were not introduced.
The method used is the convolution of the \lstinline{MCFOST} image with a two-dimensional Gaussian parametrized 
by its full widths at half maximum called beam sizes.
The final result of this process is still a 1024x1024 pixels \lstinline{FITS} image.

\subsection{Strengths and limitations}

\begin{comment}
Here I am going to discuss the strengths and limitations of numerical simulations in the context of 
disc analysis. From this subsection the need for a faster method should emerge.
\end{comment}

Computer simulations present numerous strengths which make them the preferred 
method for characterizing protoplanetary discs. First, the models simulated can be 
complex and describing the physics at a low level of abstraction
allowing to account for secondary effects and unexpected interactions which would otherwise been neglected.
They, furthermore, have a wide range of applicability as can be used, comparing the results with observations, to 
search or confirm values of most of the physical parameters characterizing the disc. 
The comparisons can be evaluated through
proper metrics that quantify the accuracy of the initial ansatz.
Unlike the analytical formulae, in this method the whole disc image and thus, in principle, the whole set of the
disc physical features are involved in the predictions that are being made.

However, the power and flexibility of the computational approach comes at some costs.
First, the fact that the whole disc modellization concur to the results might hide 
the existence of simpler and more interesting links between some parameters.
The major downside is yet the high computational cost of a simulation
which can run for hours or days. To properly investigate a parameter, the process which
is usually applied consists in executing multiple simulations varying its value and finally
comparing the results with observative data.
The need of multiple simulations makes the time needed rise up to weeks.

Last, a problem, which is worth mentioning since it affects all the characterisation methods,
is that of degeneracies afflicting disc appearances at specific
wavelengths. This means that different sets of parameters may result in the same disc structure and emission.
They might be caused by the simplifications made in the model or be intrinsically due to the disc physics. 
In the latter case the same degeneracies would also affect real observations.

\chapter{Machine learning and neural networks}

\begin{comment}
Very brief introduction to machine learning. (Birth and definition)
\end{comment}

The term machine learning was introduced by Arthur Lee Samuel in 1959
with his research on the subject applied to the game of checkers \citep{ml_checkers}.
\citet{book_ml} defined machine learning as the study of
computer algorithms that can improve automatically through experience and by the use of data.
This collection of tools and methods is usually considered a branch of artificial intelligence and 
finds its natural application in pattern recognition and data analysis tasks for which is arduous or
unfeasible the development of conventional algorithms.
The past decades have shown an incredible development in this field,
with applications in a wide variety of subjects.

In this chapter I am going to introduce a specific machine learning tool, neural networks, explaining
their key properties and pondering their application to the open issues, in the study of planet formation,
discussed in the previous chapter. I am also going to review a previous attempt at the use of this approach in the 
analysis of protoplanetary discs.

\section{Neural networks}

\begin{comment}
Basic idea behind neural networks.
\end{comment}

Artificial Neural Networks (ANN) are a group of machine learning algorithms inspired by the structure and functioning 
of biological brains.
They consist in networks of artificial neurons,
which are the elementary units capable of receiving input signals, processing
them and activating their output accordingly. The connections between the neurons account for 
the complexity of neural networks making them an extremely manifold and versatile tool:
the overall architecture and additional parameters can be tuned to design 
models specifically suited for the problems addressed.
All these elements will be explained in the upcoming sections.

Across their different applications, the two main categories of problems they are commonly used for, are 
classification and regression. In the first case, the neural network labels
each input assigning it to one of a previously defined set of categories.
A neural network that detects which animal is depicted in a given image is an example of a classifier.
In the second category of problems, the neural network is instead used to infer, from the given data, a relation
between some variables.
The models presented in the next sections are all examples of regressors.

\subsection{The artificial neuron}

\begin{comment}
Here I am going to present the perceptron model explaining:

— how outputs are generated from inputs

— what are the weights tuned during the training process

— what is the activation function
\end{comment}

\begin{figure}
    \begin{center}
        \begin{tikzpicture}
            \node[circle, draw, minimum size=1.5cm] (sigma) at (0, 0) {\huge $\Sigma$};
            \node[rectangle, draw, inner sep=0.3cm ] (f) at (2, 0) {$\bm f$};
            

            \begin{scope}[decoration={
                markings,
                mark=at position 0.5 with {\arrow{>}}}
                ] 
            \draw[postaction={decorate}] (-3, 1) node[left] {$a_0$} -- (sigma);
            \draw[postaction={decorate}] (-3, 0.5) node[left] {$a_1$}-- (sigma);
            \draw[postaction={decorate}] (-3, -1) node[left] {$a_k$} -- (sigma);
            \draw[postaction={decorate}] (sigma) -- (f);
            \draw[postaction={decorate}] (f) -- (4, 0) node[right] {$o$};
            \draw[postaction={decorate}] (0,1.2) node[above] {$\bm b$} -- (sigma);
            \end{scope}

            \path (-3, 0.5) -- (-3,-1) node [midway, sloped] {$\dots$};
            \node at (-2, 0.9) {\small $\bm{w_0}$};
            \node at (-2, 0.1) {\small $\bm{w_1}$};
            \node at (-2, -0.9) {\small $\bm{w_k}$};
            \node[above] at (1.1, 0) {$c$};

            %activations
            \draw[->] (-7, -5) -- (-3, -5) node[below] {$c$};
            \draw[->] (-5, -5.3) -- (-5, -3) node[left] {$f(c)$};
            \draw[very thick] (-6.5, -5) -- (-5,-5);
            \draw[very thick] (-5, -3.5) -- (-3.5,-3.5);
            \draw[very thick] (-5, -5) -- (-5,-3.5);


            \draw[->] (-2, -5) -- (2, -5) node[below] {$c$};
            \draw[->] (0, -5.3) -- (0, -3) node[left] {$f(c)$};
            \begin{scope}[shift={(0,-5)}]
                \draw[domain=-1.5:1.5, smooth, variable=\x, black, very thick]  plot ({\x}, {1.5/(1+exp(-5*\x))});
            \end{scope}

            \draw[->] (3, -5) -- (7, -5) node[below] {$c$};
            \draw[->] (5, -5.3) -- (5, -3)node[left] {$f(c)$};
            \draw[very thick] (3.5, -5) -- (5,-5);
            \draw[very thick] (5, -5) -- (6.5,-3.5);
        \end{tikzpicture}
    \end{center}
    \caption{Top panel: scheme of an artificial neuron.
     Bottom panel: graphs of three possible activation functions. 
     From left to right: Heaviside step function, sigmoid and ReLU.}
\end{figure}

The elementary units of neural networks, artificial neurons, are mathematical objects
which model the structure in figure 4.1: they receive a set of input values
and process them returning a single output signal.
Essentially they are parametric functions $\nu(\va a): \mathbb R ^k \rightarrow \mathbb R$ 
where $k$ indicates the dimension of the vector $\va a$, equal to the number of input connections.
To explore the arguments behind their design and unfold the
comparison with the biological unit, this relation can be rewritten 
as the composition of two functions corresponding to the two blocks in figure:
\begin{align}
    \nu (\va a) = f \circ \Sigma = f(\Sigma (\va a)), \hspace{10pt}
    \Sigma &: \mathbb R^k \rightarrow \mathbb R \\
    f &: \mathbb R \rightarrow \mathbb R \nonumber
\end{align}

The first block, $\Sigma$, performs a weighted sum of the input values $a_{j}$ introducing also a bias $b$
\begin{equation}
   c = b + \sum_j w_{j} \cdot a_{j} 
\end{equation}
where the $j$ index identifies the input connections of the artificial neuron considered.
The weights $w_{j}$ and the bias constitute the set of parameters in which
lies the potential of neural networks.
This first process mimics the ability of biological neurons to adjust the intensity of the signals in its connections
modifying the ratio of neurotransmitters and inhibitors.
Still in the biological counterpart, the total potential obtained integrating the input signals is then transmitted to the 
connected neurons if it reaches a certain threshold.
The same behaviour is modelled in artificial neurons in the second block, 
with a function $f(c)$, called activation function. It takes the $c$ value obtained in the
previous step and returns the output $o = f(c)$ which is then transmitted to the connected neurons 
becoming one of their input values.

In the choice of the function $f$, its role as an activation threshold should be kept in mind.
The bottom panel of figure 4.1 shows three examples of functions that could be chosen for this purpose.
The first and simpler is the Heaviside step function which
returns 1 if the input value is positive and 0 otherwise, simulating the discrete behaviour of 
biological neurons.
An improvement to this choice, showed at its right, can be achieved using the sigmoid
\begin{equation}
    o = f(c) = \frac{1}{1+e^{-k\cdot c}}
\end{equation}
which unlike the step function is continuous and infinitely differentiable in 0.

Last, the third panel depict the ReLU (Rectified Linear Unit) activation function which returns the input value if 
positive and 0 otherwise. This is the most popular among the whole set of activation functions available, due to its simplicity, requiring very little computation, and some mathematical properties that make the training process using gradient
descent methods particularly efficient.
Moreover, this function is non-linear, not limited and can return any real positive value.
The replacement of discreet outputs with a value varying continuously is still supported by the biological comparison.
In fact, even if real neurons can only send discreet signals deciding whether or not to activate their output, 
the frequency of transmission might vary. The continuous output thus model this possible behaviour.

\subsection{Architecture and types}

\begin{figure}
    \begin{center}
        \begin{tikzpicture}

            
            %input layer
            \node[circle, draw, minimum size=1cm] (01) at (-2, 0.75) {};
            \node[circle, draw, minimum size=1cm] (02) at (-2, 2.25) {};
            \node[circle, draw, minimum size=1cm] (03) at (-2, 3.75) {};
            \node (label_i) at (-2.5, 6) {Input layer};
            \draw[dashed, thin, gray] (-1, -1) -- (-1, 6.5);

            %hidden layer 1
            \node[circle, draw, minimum size=1cm] (11) at (0, 0) {};
            \node[circle, draw, minimum size=1cm] (12) at (0, 1.5) {};
            \node[circle, draw, minimum size=1cm] (13) at (0, 3) {};
            \node[circle, draw, minimum size=1cm] (14) at (0, 4.5) {};

            %hidden layer 2
            \node[circle, draw, minimum size=1cm] (21) at (2, 0) {};
            \node[circle, draw, minimum size=1cm] (22) at (2, 1.5) {};
            \node[circle, draw, minimum size=1cm] (23) at (2, 3) {};
            \node[circle, draw, minimum size=1cm] (24) at (2, 4.5) {};
            \node (label_i) at (2, 6) {Hidden layers};

            %hidden layer 2
            \node[circle, draw, minimum size=1cm] (31) at (4, 0) {};
            \node[circle, draw, minimum size=1cm] (32) at (4, 1.5) {};
            \node[circle, draw, minimum size=1cm] (33) at (4, 3) {};
            \node[circle, draw, minimum size=1cm] (34) at (4, 4.5) {};
            \draw[dashed, thin, gray] (5, -1) -- (5, 6.5);

            %output layer
            \node[circle, draw, minimum size=1cm] (o) at (6, 2.25) {};
            \node (label_i) at (6.5, 6) {Output layer};

            \draw (o) -- (7, 2.25);

            %connections hidden layyers
            \foreach \le [count=\li] in {2, ..., 3}{
            \foreach \lo in {1,...,4}{
            \foreach \lu in {1,...,4}{
                \draw (\li\lo) -- (\le\lu);
            }}};

            %connections input layer
            \foreach \lo in {1,...,3}{
            \foreach \lu in {1,...,4}{
                \draw (0\lo) -- (1\lu);
            }};

            %connections output layer
            \foreach \lo in {1,...,4}{
                \draw (3\lo) -- (o);
            };

            \begin{scope}[decoration={
                markings,
                mark=at position 0.5 with {\arrow{>}}}
                ] 

                \draw[postaction={decorate}] (-4, 3.75) node[left] {$a_0$} -- (03);
                \draw[postaction={decorate}] (-4, 2.25) node[left] {$a_1$} -- (02);
                \draw[postaction={decorate}] (-4, 0.75) node[left] {$a_2$} -- (01);
                \draw[postaction={decorate}] (o)  -- (8,2.25) node[right] {$o_s$};
            \end{scope}
        \end{tikzpicture}
    \end{center}
    \caption{Scheme of a feedforward dense neural network.}
\end{figure}

\begin{comment}
In this subsection I am going to explain how perceptrons are organized 
within a neural network. The concept of layer (and hidden layer) will be explained.
After acknowledging the existence of many types of neural networks
I am going to focus on the Feedforward model.

Then I am going to discuss the possibility to improve a 
feedforward neural network with convolutional layers.
I will explain how they work, what they are designed for and how they can be exploited.
\end{comment}

We have seen that artificial neurons, the building blocks of neural networks,
are fundamentally simple parametric functions.
Just as in biological brains, the complexity and flexibility
of neural networks is due to the number and nature of neurons connections.
An enormous collection of different architectures was developed since the advent of the field,
some of them are specifically designed for a certain task while others can be applied
to a wider set of problems with specific strengths and limits.

The simplest and most used are called feedforward neural networks.
Their structure can be decomposed in set of neurons, without any connection between them, called layers which are
organized sequentially in a specific order.
The first and last layers are called respectively input and output layer because of
their function in the network, they provide the interface for sending and reading data to and from 
the model. The other internal layers, without connections with external resources, are called hidden layers.
Neural networks with 2 or more hidden layers are referred to as deep neural networks and their
application constitutes the subject of deep learning.
The neurons in each layer receive, through their input connections,
the outputs produced by the neurons of the previous layer.  The information thus moves in
only one direction: forward. In a ``dense'' network all the neurons of two consecutive layers
are connected.

Maintaining the formalism previously introduced, this means that in a dense 
neural network the $n^{\text{th}}$ layer process the vector of input values 
$a_i^{(n)}$ coming from the $(n-1)^{\text{th}}$ layer producing the output vector $o_j^{(n)}$ through
\begin{equation}
    o_j^{(n)} = b_j^{(n)} + \sum_i w^{(n)}_{ji}\cdot a^{(n)}_i
\end{equation}
where $w^{(n)}_{ij}$ is the matrix containing the weights of all the connections between the current and previous layer.
The whole neural network is thus equivalent to a complex parametric function $N (\va a): \mathbb{R}^m \rightarrow \mathbb{R}^h$ where $m$ is 
the number of input variables while $h$ is the number of values returned:
\begin{equation}
    N(\va a) = o_s = b_s^{(n)} + w^{(n)}_{sr} 
    \cdot f( ... \cdot f( b_k^{(2)} + w_{ki}^{(2)} \cdot f(b_i^{(1)} + w_{ij}^{(1)}a^j)) ... )
\end{equation}
The weight matrices and the bias vectors are the parameters that can modify the functional form.

For the specific problem addressed in this thesis, convolutional neural networks could be a better alternative 
to the feedforward architecture.
This type of networks is specifically designed for image analysis, 
it is similar to feedforward neural networks with the addition
of specific layers that process the input data in order to achieve 
space invariance. With this addition, convolutional neural networks can be trained
to detect patterns independently of their position, size or orientation inside the image
resulting in more flexible and effective models.
These layers typically consists in two-dimensional filters which slide along the input image, usually of greater dimensions, 
producing a response called ``feature map'' that is then sent forward to the next layer.
In addition to the achievement of translational equivariance, a convolutional neural network
applied to images, potentially of large dimensions, reduce the number of trainable parameters
if compared to a regular neural network designed to work with the same images. This simplification improves the
performance of the model and helps avoid problems, such as overlearning, during the training process.

\subsection{Training}

\begin{comment}
Here I am going to explain the key steps of the training process.
I will explain how it works, what algorithm can be used and the concepts
of loss functions and metrics.
\end{comment}

In the context of machine learning, training algorithms are usually classified in three categories: 
supervised, semi-supervised and unsupervised.
As the name suggests, supervised machine learning algorithms use a set of pre-labelled data to 
train the model, comparing the results with the known correct answers and adjusting the parameters 
accordingly.
Instead, unsupervised machine learning models implement algorithms that are able to learn directly from 
unlabeled data. This can be achieved, for example, with the introduction of a feedback mechanism that rewards
the best solutions.
Methods which fuse parts from each approach are classified as semi-supervised. I am going to focus on the
former which is the one suggested and used in this thesis.

In order to train the model, a tool to evaluate its performance is necessary.
The ``loss function'' is introduced for this purpose. In the case of neural networks built for 
regressions, the mean squared error (MSE) of the predicted value
is usually used. However, other choices are possible.

We have already seen that neural networks are essentially complex parametric functions
which could potentially approximate any functional relation between the input and the
output space by varying the weights of the connections. The evaluation of this model
with the loss function can hence be abstracted as a function from the space of the 
network's weights to $\mathbb{R}$:
\begin{equation}
    \mathcal{L}(\va{w}): \mathbb{R}^k \rightarrow \mathbb{R}
\end{equation}
where $k$ is the total number of trainable parameters in the model.
At this point, it is clear that 
training a neural network is simply an optimization problem.
The aim is to find the set of parameters which minimizes the loss function evaluated using 
the training dataset.

For this process numerous algorithms, called optimizers, were developed, 
revolving all around the method of gradient descent
which is a technique based on the fact that the gradient of a multidimensional function, such as (4.6), computed
at a specific point, indicates the direction of steepest ascent.
To exploit this property, the process starts from a set of initial weights which is a point in the parameters space, 
then the gradient of the function to optimize is computed, and the parameters changed accordingly to reach the minimum.
After each step $t$, the weights of the neural network are thus updated through
\begin{equation}
    \va w_t = \va w_{t-1} - \eta \va \nabla \cdot \mathcal{L}(\va w_{t-1})
\end{equation}
where the hyperparameter $\eta$ called learning rate was introduced. As the name suggests, it determines 
the length of each step of the walk in the parameters space towards the minimum, varying, as a consequence,
the speed of the process. The entire algorithm is divided in epochs, each of them consists in an update of $\va w_t$
using all the data in the training set.

Finally, the computation of the gradient is itself a challenging task. The algorithm used for this purpose in the context 
of neural networks is called back propagation.

The steps unfolded here are the skeleton of almost every optimizer developed for the training of artificial neural 
networks. They then differ in some additions
made to this main algorithm to achieve better performances and increase stability.
For example, in some optimizers, the equation 4.7 is modified adding a term called momentum which is computed 
considering the previous steps and thus adding history to the algorithm. The name derives from the analogy with physical momentum.
This addition is designed to accelerate the optimization process and improve its capability.

\subsection{Training data}

\begin{comment}
Here I am going to discuss the importance of having a large dataset in the implementation
of a machine learning model. I am going to weight pro and cons of this data driven 
approach.
\end{comment}

The training process described requires a large set of data from which 
the neural network could be trained.
To implement supervised learning algorithms, this data has to be labeled, which means that
for each item representing a possible input, also the respective 
expected output has to be provided.


The dimension and quality of this 
dataset deeply influence the results obtainable from the training process.
In order to improve the results and achieve a successful model 
this dataset should contain numerous items with varying features
representing the entire hypervolume of allowed values for the variables
characterizing the input data.
There is not a proper rule to establish how many items should be contained 
in this dataset, generally they should be as many as possible, while 
the minimum required depends upon the targeted accuracy and the complexity of
the chosen model to train.


\subsection{Hyperparameters}

\begin{comment}
Here I am going to write about hyperparameters. I am going to list them
providing basics explanations about how their value can affect the model.
They will be presented as both a strength and a limitation.

This subsection should highlight the importance of carefully tuning the hyperparameters.

I am going to cite the existence of algorithms for doing this job automatically and more
efficiently than by simple trial and error.
\end{comment}

The training is an important step in the process of acquiring a successful model but, even
having access to a large and exhaustive dataset, previous choices regarding the 
building structure of the neural network might undermine the possibility to reach
satisfying results.

The parameters whose value is set during the definition of the model are called hyperparameters.
They include the type and number of layers, the number of artificial neurons for each layer and
the choice of the activation function and of the optimizer, with
its parameters such as the learning rate. In these variables lies the plasticity of neural networks as
they can be accurately tuned to build a model with the potentiality 
to address the targeted problem.

Understanding how modifying specific features alters the behaviour of the neural 
network might help in this tuning process pointing towards the right direction.
However, most of the time there are not precise prescriptions regarding the choice of these parameters
in relation to the addressed problem and they are often set by trial and error.

More sophisticated techniques and algorithms had also been developed to automate this process and 
assure a better coverage of the hyperparameters space.

\subsection{Overlearning and underlearning}

\begin{comment}
Here I am going to discuss overlearning and underlearning. 
I am going to:

    — define them

    — explain their causes

    — provide a method for their  detection 

    — discuss the solutions (early stopping, vary the number of trainable parameters, ...)
\end{comment}

\begin{figure}
    \begin{center}
        \begin{tikzpicture}

            %activations
            \draw[->] (-7, 0) -- (-3, 0) node[below] {$x$};
            \draw[->] (-6.7, -0.3) -- (-6.7, 3) node[left] {$f(x)$};
            \draw[blue, fill=blue] (-6.2, 0.2) circle [radius=0.4mm] {};
            \draw[blue, fill=blue] (-5.3, 0.5) circle [radius=0.4mm] {};
            \draw[blue, fill=blue] (-4.7, 1.3) circle [radius=0.4mm] {};
            \draw[blue, fill=blue] (-4.4, 1.8) circle [radius=0.4mm] {};
            \draw[blue, fill=blue] (-3.7, 2.4) circle [radius=0.4mm] {};
            \begin{scope}[shift={(-6.7,0)}]
                \draw[domain=0:3.4, smooth, variable=\x, black]  plot ({\x}, {0.65*\x});
            \end{scope}

            \draw[->] (-2, 0) -- (2, 0) node[below] {$x$};
            \draw[->] (-1.7, -0.3) -- (-1.7, 3) node[left] {$f(x)$};
            \draw[blue, fill=blue] (-1.2, 0.2) circle [radius=0.4mm] {};
            \draw[blue, fill=blue] (-0.3, 0.5) circle [radius=0.4mm] {};
            \draw[blue, fill=blue] (0.3, 1.3) circle [radius=0.4mm] {};
            \draw[blue, fill=blue] (0.6, 1.8) circle [radius=0.4mm] {};
            \draw[blue, fill=blue] (1.3, 2.4) circle [radius=0.4mm] {};
            \begin{scope}[shift={(-1.7,0)}]
                \draw[domain=0:3.1, smooth, variable=\x, black]  plot ({\x}, {0.3*\x*\x});
            \end{scope}

            \draw[->] (3, 0) -- (7, 0) node[below] {$x$};
            \draw[->] (3.3, -0.3) -- (3.3, 3) node[left] {$f(x)$};
            \draw[blue, fill=blue] (3.8, 0.2) circle [radius=0.4mm] {};
            \draw[blue, fill=blue] (4.7, 0.5) circle [radius=0.4mm] {};
            \draw[blue, fill=blue] (5.3, 1.3) circle [radius=0.4mm] {};
            \draw[blue, fill=blue] (5.6, 1.8) circle [radius=0.4mm] {};
            \draw[blue, fill=blue] (6.3, 2.4) circle [radius=0.4mm] {};
            \begin{scope}[shift={(3.3,0)}]
                \draw[domain=0:3.3, smooth, variable=\x, black]  
                plot ({\x}, 
                {
                    -0.00734 +
                    1.01820*\x + 
                    -1.777*\x*\x +
                    1.2559*\x*\x*\x +
                    -0.229129*\x*\x*\x*\x
                });
            \end{scope}
        \end{tikzpicture}
    \end{center}
    \caption{Three possible outcomes of a neural netwwork application to a regression problem. Blue dots 
    represent the data subject to statistical errors. The black line is the functional form 
    inferred with the neural network.
    Left panel: underlearning, middle panel: successful regression, right panel: overlearning.}
\end{figure}

The training process of a neural network could end in three different ways.
Figure 4.3 shows an example of application of three different models to the same data obtained 
from a parabolic function with the addition of small errors.
The panel in the middle represents the desired result: the neural network has been able to predict the 
functional form which originated the data without learning its statistical noise.
The panels on the sides depict instead the two issues that might occur.

The situation in the left panel is called underlearning. In this case the 
function inferred with the model oversimplifies the correct underlying relation failing 
to predict some of its characterizing features. The loss function and other metrics
should return values indicative of the neural network bad performance, providing
a way to detect this situation.
To solve this problem, as a first attempt the number of epochs in the training process is
usually increased. If a longer training does not improve the performances, the reason behind
underlearning could be the inadequacy of the chosen neural network architecture. 
Improvements can hence be obtained adding more layers or neurons, and thus raising the complexity of the model.

The situation depicted in the right panel is instead called overlearning.
In this case the problem is essentially the opposite: the model learned the training data
with its noise inferring a complicated functional form that performs
well on the training set but fails when tested with new data. In order to detect this problem,
the dataset built to train the model is divided in two subsets. The first one, the training set, is used to
train the neural network while the second, smaller, one, called test set, is used in a second phase to
test the obtained model. The training set is usually assembled with about the 80\% of the items available in the dataset.
Comparing the loss function values obtained with the train and test datasets we can
detect the overlearning. 
The possible solutions applied in this case are the exact opposite of that in the previous situation:
we can reduce the number of epochs ``early stopping'' the training process or decreasing the complexity of
the model.

\section{Strengths and limitations}

\begin{comment}
In this section I am going to discuss the strengths and limitations 
of machine learning techniques, focusing on aspects with direct relevance
to this thesis.


+ tasks in which they permorm better
+ large field of application, flexibility -> I am going to cite the approximation theorem and hyperparameters
+ data-driven: pro e cons is that no underlying relation needed just the data
- error tolerant
- unexplained behaviour
- the problem must be modelled
- generalize
- no requirements on the input data
\end{comment}

Neural networks have been applied to a huge variety of problems spanning completely different fields.
This flexibility is one of the main strengths of this class of machine learning techniques.
The universal approximation theorem has shown that 
a finite number of hidden units can approximate any continuous function to any desired degree of accuracy.
In addition, this tool has great tolerance to errors and noise in the data, and generalizes easily to
new inputs.

In order to address a specific question with a neural network two elements are necessary:
a large set of data for the training process and a model for the problem that allows
the encoding of the input and output data in numeric values readable and computable by the neural network.

At one end, these features constitutes the strengths of this tool:
as a consequence of the data-driven approach, no prior knowledge of
the studied processes is needed and any existing relation can be unraveled through
an enough complex model.
On the other end, the need for a large enough database is usually the main
downside, which complicates the application of machine learning techniques in some areas where
collecting data, or labelling them with other methods, might result difficult.
Furthermore, the behaviour of the trained model is unexplainable and 
possible simple analytical relationships between the involved variables remain hidden. 
Hence, the use of this tool does not provide new knowledge on the studied phenomenons.
In a minor set of cases encoding the problem in a model suitable for a neural network might also
result challenging.

\begin{comment}
Here I am going to discuss the computational complexity of machine learning
 algorithms in comparison with numerical simulations.

I have to highlight that the most resource requiring part is the training process.
The aim is thus to obtain a trained neural network that can be deployed and used for the study of
a wide range of different disc images without the need to re-train it.

pro: parallel processing capacity

\end{comment}

Regarding computational complexity, depending on their architecture, neural networks might require a
large amount of resources and time. They are however suited for parallel processing, even with GPUs, allowing
an improvement of performances and a reduction of the time needed.

The most demanding part is the training process because of the back propagation 
algorithm and the repeated iterations.
The forward propagation of the data can instead be performed  very quickly.
This is an advantage since the training process should be run only once to eventually deploy the trained model
and use it to analyze new data. Compared to numerical simulations, in the specific case of hydrodynamical models, 
neural networks are more convenient in terms of computational cost.

\section{Machine learning and protoplanetary discs}

\begin{comment}
In this section I am going to develop the idea of applying machine learning methods to the  study of protoplanetary
discs.
\end{comment}

Machine learning algorithms, and specifically neural networks, had been successfully used in image analysis tasks.
The flexibility along with the strengths in terms of computational cost and resistance to 
errors, makes them ideal candidates for use in the study of protoplanetary discs.

In this section, I am going to propose this new approach explaining the expected improvements and 
suggesting possible solutions to the critical points. I will then review a recent paper in which the 
authors used successfully similar methods to infer the mass of planets embedded in protoplanetary discs.
I will hence highlight the differences with our approach explaining the reasons behind our choices.

\begin{figure}
    \begin{center}
        \scalebox{0.5}{\input{images/m_diff.pgf}}
        \scalebox{0.6}{\input{images/m_diff_rad.pgf}}
    \end{center}
    \caption{Three simulations of the same disc with planets of different masses.
    Differences in the gap widths can be observed.
    Top panel: map of the flux density of the dust emission at 1.3 mm. Bottom panel:
    radial profile of the azimuthally averaged fluxes plotted in the same graph to facilitate comparison.
    Image produced using the results of simulations run for \citet{dstauv}.}
\end{figure}

\subsection{The proposed approach}

\begin{comment}
Here I am going to discuss the approach we want to propose.
 I am going to give some details about the 
suggested architecture for the neural network and what 
we expect to be able to predict with the trained
model.

I am going to think about possible scenarios which can take advantage from this approach
(ex. large surveys with lots of disc images: the neural network 
could quickly provide measures of their physical properties (?) )
\end{comment}


In the third chapter we have discussed the current methods adopted to interpret discs observations.
Among them, numerical simulations are preferred, as they provide images
directly comparable with observations in the data space and are more accurate than the analytical expressions available.
In order to run a simulation some parameters characterizing the disc dynamics and its optical properties are needed.
They have hence to be inferred from observations or, otherwise, included in the set of variables which will be varied across different simulations
and thus determined selecting the best fit to data. However, the high computational cost of hydrodynamical and radiative 
computations limits the exploration of the parameters' space.

A machine learning approach could overcome these restraints. We propose the development
of a neural network designed to read 2D images of light intensity at specific wavelengths,
which can be produced from interferometric measures, and return the inferred value of one or more 
parameters characterizing that disc. Any feature related to the disc emission or the formation of visible substructures 
has the potential of being inferrable using this technique.

Among the possible applications, I will focus on the already mentioned problem of detecting young embedded planets and measuring their mass.
Nevertheless, the same techniques could be easily generalised to other issues.
In chapter 3 I mentioned some known analytical relationships which prove an existing underlying link
between the planet mass and gaps morphologies. Figure 4.4 shows the synthetic images obtained simulating
three identical protoplanetary discs whose only difference resides in the mass of the orbiting planets. 
Even without any specific measure we can appreciate the different features of their substructures.
All these evidences suggest that a regression through a neural network might be possible.

In our perspective the trained model should be able to analyze an image, obtained from direct observation of a protoplanetary disc
presenting substructures due to forming planets, and provide measurements of the planet's properties.
In order for the neural network to perform accurate predictions, it has to be trained with sundry images of discs with
features properly varied to explore at best the ranges of their possible values. 
The training dataset can be produced collecting synthetic images generated from numerical simulations. With this method
all the physical properties of protoplanetary discs and their embedded planets are manually set, allowing the control of the 
distribution of the values explored and assuring their correctness.
Once the neural network has been trained, its application to a new input requires a few seconds which is a radical improvement
if compared to the time needed by the numerical approach.

It could be argued that since the training dataset is constructed from the results of hydrodynamical and radiative simulations,
the computational resources exploited, and does the time needed, are analogous to the use of the computational approach
which, thus, could have been used to
directly infer the desired features from the best fit. However, while this is true, we could achieve significative
advantages with two simple strategies.
First, we exploit the results of simulations already run by different people and research groups, in the past years,
for sundry independent studies. This allows the implementation of a large and diversified dataset which would have
otherwise required months and significant resources to produce the data from the ground up. Further improvements could also be obtained opening 
to the research community the possibility to contribute
uploading their own images.
Second, the neural network trained with a sufficiently exhaustive and representative dataset would be able to
successfully analyze different images without having to iterate the training phase. If applied to a large pool of observations,
the overall balance of the employed resources
would thus favour the machine learning approach. Additionally, the evaluation of each image is fast enough to allow 
the use of this tool in hypothetical large surveys of protoplanetary discs to detect the presence of planets
and infer their mass, obtaining a quick classification of the observed images. In this case, depending on the size of the survey,
the computational approach might even be unfeasible while the analytical relations available are less accurate.

\subsection{Previous attempts in literature}

\begin{comment}
Here I am going to discuss the Sayantan Auddy and Min-Kai Lin's paper citing their results
and explaining the main differences with the approach we propose.
\end{comment}

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{images/4.5.fig7.auddy&lin/image.jpg}
    \end{center}
    \caption{Results obtained by S. Auddy and M. Lin in the application
    of a neural network aimed at inferring the planet mass from some features of the discs
    and gap where it is embedded \citep{Auddy_2020}. A comparison with the application of \citet{Lodato_2019} and 
     \citet{kanagawa} analytical formulae is also shown.
    Image reprinted from \citet{Auddy_2020}.}
\end{figure}

Some machine learning applications in this field have been already tried, 
with an approach similar to the one proposed. For example,
\citet{Auddy_2020} realised a training dataset with 976 two-dimensional simulations varying, in addition to the mass of the embedded planet, the values of the $\alpha$-viscosity, the aspect-ratio, the Stokes number, the gas to dust ratio and the index of the power law used to model 
the initial density. 
They then measured, in the obtained results, the width of the gaps formed in the
dust component due to its interaction with the orbiting planet. A neural network
was thus trained to infer the planet mass from this last measure and the other parameters previously mentioned.

The results achieved were promising. They compared the mass value predicted by the neural network
with the results obtained applying the analytical formulae 3.1 and 3.2.
In picture 4.5 we can observe this comparison.
The neural network performed better than both the analytical approaches: its prediction error
follows a normal distribution whose mean value and standard deviation are both smaller showing its
better accuracy and precision.

The approach we propose in this thesis present substantial differences. 
First, we aim at reducing at its minimum the number of disc features needed and not 
directly obtainable, with low uncertainty, from observations.
For this reason, we opted for a neural network designed to receive the entire
observed image as input which also allows, as a fortunate consequence, to consider the entire morphology of the disc substructures.

Additionally, they considered only 2D simulations to reduce the computational power needed
while we built our dataset with 3-dimensional results which allowed us to account for additional parameters, such as the different 
inclinations of the disc. We also included in the training set, images of the same simulation
produced at different times, thus introducing an additional variable.

\chapter{Dataset design}

\begin{comment}
In this chapter I am going to unfold the main part of my work: the design and implementation of the dataset.
Here I am going to recall the key features of a good dataset for machine learning.
\end{comment}

Every machine learning application depends heavily upon the training dataset. 
Its realization is the most crucial aspect of the process and, in addition to collecting the data,
involves designing how to organize and store them and checking their quality, 
ensuring that the entire parameter space is properly represented and that no bias was introduced during
the production and selection stages.

In the approach we proposed, the aim is to build a database of synthetic images of protoplanetary discs
probed through their thermal dust emission, produced exploiting 
the results of already run simulations. The design and implementation of this dataset is the heart of this
thesis. In this chapter I am going to present its content and the structure developed to organize and store the information. 
The scripts produced to support the use and maintenance of the database will also be explained, 
providing an overview of their functions.

\section{The data}

\begin{comment}
In this section I am going to present what the dataset is made of: fits images and the list of parameters included in the fits files 
and in data.js. I am going to briefly explain why they were included and their possible use.

I am also going to present images showing the data distribution over the parameters space, discussing
 which of them are properly explored and which not.
\end{comment}

As I already mentioned, the starting data from which we built the database 
we designed to train neural network models are results
of hydrodynamical and radiative simulations.
At this stage of the simulation process, the resulting images
are spatially resolved maps of the flux density of light at a specific wavelength as seen 
from a given relative position.
This fictitious images are further elaborated with \lstinline{pymcfost},
whose functioning has been discussed in chapter 3, to simulate the limited resolution of
the telescopes through a convolution with a Gaussian beam.
The final product is a set of images in the \lstinline{FITS} format. In the next section I will explain how they are organized
and indexed, while here I am going to discuss the content of each image providing insights on the source and
possible usages of each parameter included. I will also present some global features of the dataset.

The \lstinline{FITS} format encode the stored information in Header and Data Units (HDUs).
Each one, as the name suggests, consists in an ASCII text header organized in key-values tuples, followed 
by a binary, usually multidimensional, array where the data is stored.

In our dataset, each \lstinline{FITS} file contains only one image corresponding to a single HDU.
In the header, some parameters of the disc images depicted are stored. 
The preprocessing step, run on the files obtained from the radiative transfer simulations, ensures
the proper standardization of the header, retrieving and adding the missing variables.
Table 5.1 shows the list of the header keys corresponding to each of these parameters
saved in the \lstinline{FITS} files. Brief descptions are also provided.

\begin{table}
    \begin{center}
        \begin{tabular}{l r}
        \toprule
        KEY & content \\
        \midrule   
        %hydro simulations        
        RIN & inner radius of the hydrodynamical simulation domain\\
        ROUT & outer radius of the hydrodynamical simulation domain\\
        RC & reference radius for the exponential taper in the radial profile density\\ 
        MGAS & total mass of the gas component\\
        MDUST & total mass of the dust\\
        HRIN & aspect ratio at RIN \\ 
        RHOG & intrinsic grain density\\ 
        PINDEX & power law index for the initial surface density profiles\\
        QINDEX & power law index for the initial sound speed profile \\
        TIME & time of the snapshot in complete orbits of the planet \\
        ALPHASS & Shakura-Sunyaev $\alpha$-viscosity\\
        ECC & eccentricity of the planet's orbit\\\\

        %star(s)
        NSTARS & number of stars in the simulation \\
        TSTAR\# & temperature of the star (K)\\
        MSTAR\# & mass of the star in solar masses \\
        RSTAR\# & radius of the star in solar radius \\ \\
        
        %planet(s)
        NPLANETS & nnumber of planets in the simulation\\
        MPLANET\# & planet's mass in Jupiter's masses \\
        RORB\# & radius of the planet's orbit\\ \\
        
        %observational parameters
        DISKPA & disc's position angle\\
        INCL & disc's inclination\\
        DISTANCE & disc's distance \\
        WAVE & wavelenght of the radiation showed in the image\\
        GDRATIO & gas-dust ratio used in the MCFOST simulation \\ \\

        %units
        BUNIT & units of the data in the image \\ 
        CDELT1 & x pixel scale (deg) \\
        CDELT2 & y pixel scale (deg)\\ \\
        
        %convolution beam's shape
        BMAJ & major FWHM of the gaussian beam \\
        BMIN & minor FWHM of the gaussian beam\\
        BPA & beams's position angle \\ \\
        

        %computable
        %TEMP & gas temperature at RIN\\
        %GRSIZE & grain size ($\lambda/2\pi$)\\
         \bottomrule
        \end{tabular}
        \end{center}

        \caption{List of keys of the properties stored in the header of the \lstinline{FITS} files
        collated in this thesis' database. For each key, a brief description is provided explaining which 
        feature the respective field should contain.}
\end{table}

In this table, I divided the list in six different groups. The first one contains
variables which are read from the \lstinline{PHANTOM}'s dump file and are thus hydrodynamical features of
the disc. The next two groups list the physical properties of stars and planets. These groups contain the 
``multiple keys'', those in the \lstinline{KEY#} format, which appear more than once, substituting the \lstinline{#} character with an index,
in case of multiple stars or planets.
Except for the star temperature which is a parameter of the radiative transfer simulation, also these variables are retrieved from
the saved standard output of \lstinline{MCFOST} which reads these properties in the \lstinline{PHANTOM}'s dump file.
The fourth set collects some input variables of the \lstinline{MCFOST}'s simulation.
They set the relative position and orientation of the disc and the wavelength of the radiative emission probed.
The \lstinline{GDRATIO} variable contains the value of the gas to dust mass ratio set when running \lstinline{MCFOST}
which is simulated
independently to the value of the same parameter used to obtain the respective \lstinline{PHANTOM} dump. For
this reason computing \lstinline{MGAS/MDUST} gives a different result since these last parameters refer to the results 
of the \lstinline{PHANTOM} simulation.
This method is preferred due to the less time required to run a radiative transfer simulation with respect to a
hydrodynamical one. The obtained results are valid in the assumption of negligible back reaction between gas and dust which
is usually true for sufficiently large gas to dust ratios ($\gtrsim 10$).

Finally, the fifth group provides the units of the stored data while the last one contains
the dimensions and orientation of the Gaussian beam used to convolve the image.

The images collected within the database in the version deployed with this thesis, come from two different sources.
The first group of data, provided by Dr. Benedetta Veronesi, consists in numerous simulations,
both hydrodynamical and radiative,
of the DS Tau disc \citep{dstauv}. 
I additionally re-run the radiative transfer through MCFOST to obtain images of the disc at different inclinations.
From this source, after the further processing through pymcfost, we obtained a total of 20370 images.
The second group of data was provided by Dr. Enrico Ragusa and consists in sets of snapshots
obtained from hydrodynamical simulations from which I selected only the ones which showed the presence of gaps.
I then simulated the radiative transfer and produced images with different gas to dust ratios and inclinations.
From this source we eventually obtained 4350 images.

For each result of a radiative transfer simulation, I used \lstinline{pymcfost} to produce 10 images obtained convolving circular 
Gaussian beams of different sizes (FWHM)
logarithimically spaced between 0.01 and 1 arcsec. Figure 5.1 shows an example of the results obtained.
These values were chosen to explore resolutions
realistically achievable, as proved by for example the DSHARP project \citep{dsharp}, and additionally explore the upper and lower limits.
Very resolute images are included for applications where it might be preferable to remove instruments limitations
or to test models in the perspective of future improvements in the observational techniques. 
On the other end the less resoluted images are convolved with beams up to about the same dimensions of the disc. 
This limit may be useful to test what information could be extrapolated from spatially unresolved observations
which essentially provide only measures of the light intensity but can not reveal the disc substructures.

\begin{figure}
    \begin{center}
        \scalebox{0.4}{\input{images/5.1.res.diff/res_diff.pgf}}
    \end{center}
    \caption{The ten different telescope resolutions reproduced in our database 
    convolving beams of different sizes for each synthetic image. 
    In this figure all the images are obtained from the same simulation result varying only the
    beam size which
    is shown in the bottom left corner.}
\end{figure}

The final product is a database made of 24720 \lstinline{FITS} images with dimensions of 1024x1024 pixels. In this
version each image contains only one star and one planet and exhibit an induced gap.
Among the parameters listed in Table 5.1 only some of them vary significatively across different simulations.
Apart from the beam sizes whose distribution has already been discussed, they are, the gas and dust masses, the gas to dust ratio, 
the planet's mass and orbital radius and the time. Picture 5.2 shows the distribution of these parameters and the scatter
plots showing the correlations between these variables. Some of them are easily interpretable, for example the
gas and dust masses decrease with time accompanied by the planet's mass increase can be understood as consequence of the planet's accretion
process. There is also a strong correlation between the planet's orbital radius and its mass which is due to the third Keplerian law.

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{images/pairplot.png}
    \end{center}
    \caption{Pairwise relationships in the deployed dataset. The diagonal plots show how the values of each parameter are distributed
    while the off-diagonal plots highligth correlations among these variables. Only the parameters whose values vary
    significantly within the dataset are shown.}
\end{figure}

Most of the discs were captured, in the images, from three different inclinations of approximately
0, 30, and 60 degrees. Furthermore, all of them were computed for the dust continuum emission 
at 1.3 mm wavelength.
The planet's mass, which is the target feature of the possible application on which we focused, shows an acceptable distribution over
a sufficiently large interval of values. The gas to dust ratio is another parameter which has been widely explored.

\section{Structure and interface}

\begin{comment}
Here I am going to discuss the structure we designed for the db and the interface provided to 
access the data.
\end{comment}

Every image presented in the previous section is saved as a different file whose name is in the format 
described by the regular expression 
\begin{center}
    \begin{lstlisting}
        ^\d{6}_.{3,5}TIME\d+GD\d+W\d+B\d+.fits$
    \end{lstlisting}
\end{center}
which provide some information about the parameters used to produced that image. The first six 
characters are numbers forming an index key which is used to uniquely identify each image.
The collection is stored entirely inside a single directory where two additional files are generated: 
\lstinline{index.js} and \lstinline{data.js}. They are json files aimed at indexing all the information stored in the \lstinline{FITS} 
files except for the images' data, in order to provide an easier access especially when the dataset is not completely
available locally.

The entire dataset is, in fact, stored in a public github repository exploiting
the git lfs storage system, and the typical usage workflow for which it was designed 
consists in the following steps. First, through a script provided, and explained in the next section,
the index files can be downloaded. They can then be read to obtain a dictionary which is loaded in a pandas DataFrame containing
a reference to each image and all the parameters stored in their headers. 
It is thus possible to analyze this data to select, in case, a specific subset of the entire database
according to the implemented application.
The keys of the images filtered in this way can thus be used to download them through the provided script, without the need
to get the entire dataset whose size approaches hundreds of gigabytes.

To allow these functionalities, inside the index file, some additional parameters regarding each image are stored.
They include the file names, their hashes and the unique keys associated to each item.

\section{Supporting scripts}

\begin{comment}
Here I am going to write about the scripts I wrote which allow the user to handle the database
and preprocess the results coming from MCFOST simulations.
\end{comment}

To populate and manage the database, I developed the python package \lstinline{dblib}
and some command line scripts which provide immediate access to their main features.
The structure of the package is the following:

\begin{minipage}{\textwidth}
\dirtree{%
.1 dblib. 
.2 index\_keys.py. 
.2 index.py. 
.2 pymcfost\_process.py. 
.2 utils.py. 
.2 \_\_init\_\_.py. 
}
\end{minipage}

These modules fulfill three main functions: preprocessing through \lstinline{pymcfost} and standardization
of the images obtained after the \lstinline{MCFOST} simulation, generating and accessing
the index files and last, downloading the database, entirely or partly, from the remote host.

The preprocessing is handled by the submodule \lstinline{pymcfost_process.py} which contains the function 
\lstinline{read_dirs_doimages()} that reads the \lstinline{FITS} files produced by \lstinline{MCFOST},
and some log files containing the parameters of the simulations, all  organized in a specific structure of subdirectories.
This function thus convolves the images with Gaussian beams of given sizes and adds some of the read parameters to the header
file to reach the complete list in Table 5.1.
Therefore, we used this module to preprocess and standardize the previously computed
simulations collected, before adding them to the final database.
In order to simplify the use of this tool, I wrote a script called \lstinline{pymc_prepro} which
allows the use of the functions just mentioned through a command line interface (CLI).

The second function of these scripts is the creation and handling of the index.
The two submodules \lstinline{index.py} and \lstinline{index_keys.py} fulfill this purpose.
They provide functions
that read the parameters stored in the \lstinline{FITS} files and generate or update the
index. Furthermore, they allow to access the data stored in these index files,
\lstinline{index.js} and \lstinline{data.js}, 
through the function \lstinline{get_data()}
which merges their content together, returning a python dictionary.
This dictionary can then be passed to a function of \lstinline{pandas},
a python library designed for data analysis and manipulation, obtaining a more versatile pandas \lstinline{DataFrame}.
The generation of the index files can be done through the CLI script \lstinline{generate_index} which has to be run from 
the terminal specifying the directory where the unindexed \lstinline{FITS} images are stored.

Last, the third function of these scripts, access and retrieving the dataset from the remote server where
it is hosted, is carried out by the \lstinline{downloader} module.
It implements functions which are able to determine the remote location of specific items of the database 
given their unique keys, download them and check whether the download has succeeded.
The code developed employs the \lstinline{https} protocol to get the data and compute the \lstinline{sha1} 
checksum to check the integrity of the files retrieved.
These functions are interfaced by the CLI script \lstinline{get_db}.

\section{Expanding the dataset}

\begin{comment}
    Here I will explain how the tools provided allow to easily expand the dataset. 
    Then I am going to give future perspectives on how the database could be improved.
\end{comment}

The version of the dataset deployed with this thesis is not intended to be the final version. 
We have seen, in the previous section, that only a small set of all the parameters exhibit different values distributed
across a sufficiently large range. Variations of the other ones could be explored in future updates.

In the development of the database structure, great effort has been put into making the addition of new
data as easy as possible. The previously discussed scripts were also developed with this purpose.

In order to integrate new images, \lstinline{FITS} files obtained from a radiative 
transfer simulation done with \lstinline{MCFOST} or a similar tool are needed. If results of hydrodynamical 
simulations are available, they also can be used, additionally running the radiative transfer simulations.
Before using the \lstinline{pymc_prepro} script to preprocess the images and convolve them with the Gaussian beam,
they need to be organized in a specific tree of directories.
Each group of images with the same values for these parameters

\begin{center}
    \begin{tabular}{lr}
        \toprule
        key & description \\
        \midrule
        RIN & inner radius of the hydrodynamical simulation domain \\
        ROUT & outer radius of the hydrodynamical simulation domain\\
        RC & reference radius for the exponential taper in the radial profile density\\ 
        HRIN & aspect ratio at RIN \\ 
        RHOG & intrinsic grain density\\ 
        PINDEX & power law index for the initial surface density profiles\\
        QINDEX & power law index for the initial sound speed profile \\
        ALPHASS & Shakura-Sunyaev $\alpha$-viscosity\\
        ECC & eccentricity of the planet's orbit\\
        \bottomrule        
    \end{tabular}
\end{center}

have to be grouped inside the same directory organized according to the following substructure:

\dirtree{%
.1 parent\_dir. 
.2 MP1\_time050. 
.3 gd50. 
.4 data\_th. 
.4 data\_1300. 
.5 RT.fits.gz. 
.5 disc.para. 
.4 image.log. 
.3 gd70. 
.3 gd100. 
.3 {...}. 
.2 2HoR\_time100. 
.3 {...}.
.2 MP25\_time150. 
.3 {...}.
.2 {...}.
}


The different simulation are thus divided in subdirectories according to the planet's mass simulated, the time of the
snapshot and the gas to dust ratio. Additionally, inside each of the \lstinline{gd#} directories 
there might be different subdirectories named \lstinline{data_#} automatically created by \lstinline{MCFOST} 
and containing results at different wavelengths (indicated by the \# in $\mu$m).
Apart from the \lstinline{RT.fits.gz} file produced by the \lstinline{MCFOST} there are two other files needed as it can be seen
from the previous scheme: \lstinline{image.log} contains the redirected standard output of the \lstinline{MCFOST} run which 
provides the parameters read from the phantom file,
\lstinline{disc.para} is instead the input file used to run that simulation.
When all the data available are properly grouped in this way, the \lstinline{pymc_prepro} script
can be run. If some of the features in the last table vary significatively across multiple simulations this script 
results inconvenient. In this case the functions implemented in the \lstinline{dblib} package can be used
to build a new script adapted to
this situation.

Once the \lstinline{FITS} images have been preprocessed they can be added to the local directory containing the dataset 
and indexed running the script \lstinline{generate_index} with the option \lstinline{--update}. 

At this point the database has been expanded. To share the contribution it is possible to merge it to the public remote database
opening a pull request in the hosting repository. 

The current version of this database can be improved under many aspects. The first suggestion
is, as I anticipated, the addition of simulations exploring ranges of the parameters which do not vary across the images already present.
For example, some hydrodynamical variables such as the $\alpha$-viscosity 
or the density radial profile could generate different pictures which could then be interesting to probe with a neural network.
Furthermore, we only included images of discs where the embedded planet interacted with the medium inducing the formation of
gaps. It might be interesting to include in the dataset also simulations where the gap does
not form, both with and without a planet, or, on the contrary, where we observe gaps without the presence of planets.
In this way, these images might be used, for example, to train a neural network to detect planets and investigate
the existence of ways to confidently distinguish planet induced gaps from substructures due to different phenomena. 

Images of discs with double stars or more than one planet might also be included, together with examples
of the other possible substructures in order to include all the potential morphologies.
With these additions it may also be necessary to update the headers of these files including some flags which would allow 
a more selective filtering of the data depending on the needs.

\chapter{Proof of concept}

\begin{comment}
Here I am going to present an example of model trained to predict planet's mass from 
disc images.
From the different models I tested (and I will test)
I am going to report here the one which gives the best result.
I am also going to cite the python libraries used to implement this model (TensorFlow).
\end{comment}

In order to prove the feasibility of our approach, we used the collected data to develop and train a neural network
aimed at inferring planets' masses from the sole flux density map of the dust continuum emission.
The model was implemented using TensorFlow's libraries.

In this chapter I am going to present the trained model, providing a usage example of
the dataset developed. In this context I will show its versatility in the selection 
of subsets of data, and the simple 
steps required to interface it with standard TensorFlow's neural network models.
The obtained results will be reviewed discussing the successfull achievements and the emerged limits.
Finally, I will try to reach the final purpose of this approach applying the trained
model to observations of real discs.

\section{Data pre-processing}

\begin{comment}
Here I will explain how I chose the data used during the training process.
This section should highlight the versatility of the dataset showing the possibility to split the 
data according to our needs.
\end{comment}

The entire dataset developed contains 24720 images for a total size of approximately 190 GiB.
In order to run the training process in reasonable times, remaining able to explore different configurations, we 
had to select a subset of the entire collection of data. In addition to the overcome of computational limits, 
this selection is also useful to reduce the redundancy of the entire dataset which, in its completeness, cotains
several images of the same disc convolved with different beam sizes
that simulate different resolutions of the observative tools.
These images were in fact, by design, intended to be filtered according to the expected resolution of the real
images object of study.

In this specific case I selected the images convolved with a beam dimension of 0.046 arcsec which is,
among the available sizes, the closest to the currently achievable best resolution according to the DHSARP project \citep{dsharp}.
To further reduce the size of the dataset, I also selected only images with inclinations of about
60° which constitutes the larger set.
As a result, I obtained a set of 1024 images which was randomly split in two subsets for the training and
test phases containing respectevely the 80\% and 20\% of the whole set.
The test set contains images which are not used during the training process allowing the evaluation
of the model on never seen data.

With the aim of showing how the database developed can be used in machine learning applications,
I am going through the steps required to filter the data reporting the lines of code used.

First, the dataset index has to be imported and loaded using 
the function \lstinline{dblib.index.get_data()} 
developed for this purpose, which returns
a python dictionary. It that can then be loaded in a pandas \lstinline{DataFrame}
which allows a simpler filtering using pandas queries:

\begin{lstlisting}
    import pandas as pd
    import dblib.index as index

    data = pd.DataFrame(index.get_data())
    data = data.set_index('KEY')
    trial_data = data[(data.BMAJ>0.046) & (data.BMAJ<0.048)
                        & (data.INCL>58)][['FILENAME', 'MPLANET0']]
\end{lstlisting}

In these lines, the selection previously discussed is applied to the entire dataset and,
for each item, only the name of the file that stores the disc image and the mass of the embedded planet are kept, as
all the other parameters will not be used. However, if after the training process, for any reason, becomes necessary
to access the other variables they could be retrieved using the KEY parameter which, in line 5, is set as 
the index column.

The following step requires loading in memory all the images. This operation is achieved through the following
instructions

\begin{lstlisting}
    from astropy.io import fits
    import numpy as np

    def get_image(filepath):
        with fits.open(filepath) as hdu:
            return hdu[0].data
    
    train_images = np.array(
                    [get_image(f"../../database/{filename}") 
                    for filename in trial_data['FILENAME'].tolist()]
                    )
\end{lstlisting}

where the defined function \lstinline{get_image()} opens the \lstinline{FITS} file 
and extracts
the data using the \lstinline{io.fits} module of 
the \lstinline{astropy} package. All the images are thus
stored in a \lstinline{numpy} three-dimensional array 
with shape \lstinline{(n, 1024, 1024)} where \lstinline{n} is the total number of images loaded.

The last step, before the split in training and test sets, consists 
in the reshaping of this array. The  
function \lstinline{get_image()} return a 1024x1024 matrix of float containing
the value of the flux density at each pixel. 
Tensorflow models require each 2D image to be, instead, a matrix of arrays.
The additional dimension of the obtained tensor is introduced to account for 
different channnels allowing, for example, the analysis of RGB images where 
the intensity, at each pixel, of the three colors Red, Green and Blue is given.

The data of the images stored in our database has only one channel. Therefore,
the array can be reshaped through

\begin{lstlisting}
    train_images = train_images.reshape(train_images.shape[0], 
    train_images.shape[1], train_images.shape[2],  1)
\end{lstlisting}

obtaining the new array \lstinline{train_images} with the shape \lstinline{(n, 1024, 1024, 1)}
where \lstinline{n} is the total number of items in the set.
This array can be directly passed to the \lstinline{TensorFlow} function that initiates
the training process.

These are all the steps required to retrieve the database we developed, filter it according to the application
and interface the data with standard and popular python packages.

\section{Adopted model}

\begin{comment}
Here I am going to explain the model used: number of layers, type of neural network, activation functions,
optimizer, metrics.
\end{comment}


\begin{figure}
    \begin{center}
        \begin{tikzpicture}
            
            \node[rectangle, draw, minimum width=3cm, thick, inner sep=0cm] (input) at (0, 0) {\includegraphics[width=3cm]{images/inp_mod.png}};
            \node[rectangle, draw, inner sep=1.5cm, thick, fill=gray, fill opacity=0.5 ] (norm) at (4, 0) {};
            \node[rectangle, draw, minimum height=8cm, minimum width=0.3cm, thick, fill=gray, fill opacity=0.5] (flat) at (7,0) {};
            \node[rectangle, draw, minimum height=3cm, minimum width=0.3cm, thick, fill=gray,  fill opacity=0.5] (h1) at (8.5,0) {};
            \node[rectangle, draw, minimum height=1.5cm, minimum width=0.3cm, thick, fill=gray,  fill opacity=0.5] (h2) at (10,0) {};
            \node[rectangle, draw, minimum height=0.3cm, minimum width=0.3cm, thick, fill=gray,  fill opacity=0.5] (output) at (11.5,0) {};
            \draw[->, gray, thin] (output) -- (12.5, 0) node[right, black] {$M_p$};
            \draw[color=gray, thin] (input.north east) -- (norm.north west);
            \draw[color=gray, thin] (input.south east) -- (norm.south west);
            \draw[color=gray, thin] (norm.north east) -- (flat.north west);
            \draw[color=gray, thin] (norm.south east) -- (flat.south west);
            \draw[color=gray, thin] (flat.north east) -- (h1.north west);
            \draw[color=gray, thin] (flat.south east) -- (h1.south west);
            \draw[color=gray, thin] (h1.north east) -- (h2.north west);
            \draw[color=gray, thin] (h1.south east) -- (h2.south west);
            \draw[color=gray, thin] (h2.north east) -- (output.north west);
            \draw[color=gray, thin] (h2.south east) -- (output.south west);

            \node at (0, 1.8) {1024x1024};
            \node at (4, 1.8) {1024x1024};
            \node at (7, 4.3) {1,048,576};
            \node at (8.5, 1.8) {32};
            \node at (10, 1.05) {16};
            \node at (11.5, 0.45) {1};

            \node at (0, -1.8) {Input};
            \node at (4, -1.8) {LayerNormalization};
            \node at (7, -4.3) {Flatten};
            \node at (8.8, -1.8) {Dense};
            \node at (10.1, -1.05) {Dense};
            \node at (11.6, -0.45) {Dense};

        \end{tikzpicture}
    \end{center}
    \caption{Scheme of the neural network model implemented for the proof of concept.
    For each layer its dimension and type are indicated respectively above and below its representation.
    This model has a total of 33,557,057 trainable parameters.}
\end{figure}

This first machine learning application was primarly designed with the purpose of probing the limits and potentials
of this new approach. For this reason, we chose the simplest model with the ability to solve the problem addressed among the
sundry alternatives availables.

More specifically we implemented a feedforward, densely connected, neural network with two hidden layers
made of respectively 32 and 16 neurons and an output layer with a single neuron that returns the predicted value of the planet's mass.
The ReLU activation function was used in each unit, except for the output one, to introduce non-linearity.

The input interface is made of two layers with specific functions.
The input images, containing data organized in two dimensions, are provided to the first layer which is able to standardize the data 
subtracting the mean value and dividing for the standard deviation.
Tensorflow provides two different layers which implement this process with specific differences.
The first one is called \lstinline{LayerNormalization} and computes the mean and standard deviation
values indipendently for the flux density data of each image which are thus separately
standardized.
The alternative is instead called \lstinline{BatchNormalization}. During the training process, it works
using the mean and standard deviation computed with the data of all the images in the current batch of inputs. 
Batches are groups in which the items in the dataset are divided during 
the training process, their size can be modified as a hyperparameter.
After the training process, when the model is applied to the validation set or to new data, 
this layer normalizes the input using a moving average of the means and standard deviations of the batches it has seen during training.
The first option basically focus on the flux density variations inside each image enlightening the presence of substructures, such as gaps, 
but, on the other end, removing the information carried by the absolute values of the flux density which are essentially measures
of the dust mass. This information, more precisely the relative difference of the dust mass among different discs is instead preserved 
with the second option which, however, has the risk of reducing the fluctuations inside each image potentially hiding the existence
of substructures.
Following these arguments, we expected the first option to produce better results and some tests confirmed this hypothesis.
Therefore, we used the \lstinline{LayerNormalization} layer in the implementation whose results are discussed in this chapter.

After the normalisation, the data, organized in two-dimensional matrices are flattened by a layer aimed at this purpose, before
eventually, reaching the hidden layers.

The optimizer used is \lstinline{Nadam} which is analogous to the \lstinline{adam} optimizer but implemented with a Nesterov momentum.
The \lstinline{adam} optimizer is a stochastic gradient descent method with the addition of first order and second order moments
that are adaptively estimated during the training process. According to
 \citet{nadam},
 the method is ``computationally efficient, has little memory requirement, 
 invariant to diagonal rescaling of gradients, 
 and is well suited for problems that are large in terms of data/parameter''.
The Nesterov momentum algorithms differ from the traditional ones essentially for where, at each step, the gradient is evaluated.
Using a Nesterov momentum its contribution to the motion in the parameter space is considered before computing the gradient 
which is evaluated in the new point reached instead of the original one. This simple modification usually boosts the training process allowing
a faster convergence.
We also lowered the learning rate reaching the value $10^{-4}$ to mitigate some undesired effects encountered.

All these hyperparameters, including the number of hidden layers and of their neurons, were accurately chosen
with a trial and error approach. Here, we presented the model which showed the best performance preserving a relative simplicity
and thus economic computational requirements. 
Figure 6.1 shows an overview of its main features.

To evaluate the model, both during and after the training process, we used, as the loss function, the mean squared error of the predicted mass values 
with respect to the ones known from the simulation files and stored in our dataset.

\section{Results}

\begin{comment}
Here I will discuss the results obtained.
\end{comment}

\begin{figure}
    \begin{center}
        \input{images/loss.pgf}
    \end{center}
    \caption{Trend of the loss function (mean squared error) 
    evaluated on the training set (black line) and the test set (blue line)
    after each epoch. Both the curves converge towards 0
    indicating that the neural network has been successfully trained to predict the planet's masses 
    from images showing discs' morphologies. }
\end{figure}

\begin{figure}
    \begin{center}
        \scalebox{0.85}{\input{images/corr&err.pgf}}
    \end{center}
    \caption{Left panel: correlation between the predicted mass and the real value obtained from the simulation files
    for all the items in the test set. The black lines draws the $M_{pred} = M_{sim}$ curve.
    Right panel: distribution of the relative prediction error $(M_{pred} - M_{sim})/M_{sim}$ 
        computed for the test set. The blue box 
    zooms in the -10\% to 10\% range which contains the 89\% of all the items. This distribution exhibits mean and standard 
    deviation of respectively 5\% and 27\%. The root mean squared error is $0.15 M_{jup}$ which will be assumed
    as the uncertainty of the predictions inferred with this model.}
\end{figure}

The training of the previously presented model took about 5 hours of wall time running on a laptop equipped with a dual core CPU
with 2GHz clock frequency and an NVIDIA GeForce 840M GPU. Additionally considering the opportunity to run
this process on a faster and better performing machine, the computational time required by the training of a neural network
model appears to be significantly smaller than that needed for numerical simulations. We thus proved the
first element in support of machine learning approaches.

The neural network was trained for 200 epochs. The trends of the loss function evaluated on the training and test sets after each epoch are shown
in figure 6.2 where their drop, after approximately 125 epochs can be appreciated.
Furthermore, there are not evident signs of underlearning or overlearning as the loss function keeps low values for both the samples.
More precisely, low values of the mean squared error computed predicting the masses of planets in the test set images 
show the successfull functioning of the trained model on new unseen data.

This graph can, alone, establish the fullfilment of this first application purpose which was to evaluate the feasibility of
a machine learning approach. However, we further analyzed the obtained results to provide 
an estimation of the model's accuracy also comparing it with the results obtained by Auddy and Lin with their
approach. 

The root mean squared error evaluated on the test set could be assumed as the
prediction uncertainty of the trained model which is $0.15 M_{Jup}$ corresponding to respectively
the 15\% and 1.5\% of the less and the most massive planets
for which it has been tested. Auddy and Lin achieved with their model an uncertainty estimated, with the same method,
in the 11\% 
of the most massive planet on which they tested their network. 
Due to the different ranges of planet's masses adopted the comparison is possible only through the relative error.


The left panel of figure 6.3 shows the correlation between the predicted and simulated masses of the validation set which
lie along the $M_{predicted} = M_{simulated}$ line indicating a strong correlation.
We also computed the relative prediction error $(M_{pred} - M_{sim})/M_{sim}$ for each item
in the test set. The right panel of the same figure illustrates its distribution
presenting a mean value and standard deviation respectively of 5\% and 27\%.
The trained model is afflicted by a slight bias which tends to overestimate the planet masses.
Furthermore, the relative error distribution presents a large standard deviation which indicates the great
variability of this quantity in its evaluation among the test set. For instance there are a few cases afflicted by
prediction errors up to 200\%.

There are hence some margins for improvements partly due to the reduced size of
the dataset and the low complexity of the neural network employed. With more sophisticated models and
a larger dataset we are confident that better results could be obtained.

Finally, we also tried to apply the trained model in a real-case scenario. In this last test we used
the ALMA observation of the DS Tau disc in the dust continuum at 1.3 mm.
The image, obtained processing the raw data, was produced by \citet{Long_2018}.
We further adapted it to meet the same specifics of the ones collected in the employed dataset.
First it was cropped to
approximately the same size considering the pixel scale for each axis.
We then checked that the data was in the same units and upscaled the image using
\lstinline{scikit-image} functions to have the same resolution of the images in the training set (1024x1024).
The obtained matrix of data was thus passed to the trained model which predicted a planet's mass of $M_p = 3.3 \pm 0.15 M_{Jup}$.
Using numerical simulations \citet{dstauv} established the presence of a planet, hidden in the gap, with an estimated
mass of $3.5 \pm 1 M_{Jup}$. The two results are thus compatible.

\chapter{Conclusions}
\section{Conclusion}


The main goal of this thesis was the design of the dataset for machine learning applications,
which was eventually deployed and made publicly available in its first version. A total of 24720 different synthetic
images of protoplanetary discs were produced and collected starting from the results of two groups of numerical simulations.
We obtained a significant representation of the different values of the gas to dust ratio, the age of the discs,
their inclination and the masses of the embedded 
planets. We also convolved the images with gaussian beams of varying sizes to reproduce a set of possible limited resolutions
of the telescopes used for real observations. 
Some additional parameters are also explored with less significant distributions.
Along with the storage structure we also developed a python package designed to handle the dataset
running the main operations required for its manteinance and usage. 
Some python scripts are provided to interface these functions
from the command line.

We showed an example of usage of the dataset and explained the steps required to expand its content
highlighting the simplicity of the steps required for these activities.
We additionally explained how the dataset can be interfaced with modern python packages
well known and daily used in the field
of machine learning, i.e. TensorFlow and Pandas.

The results obtained with the proof of concept are promising. The trained model successfully
characterized the planets embedded in the protoplanetary discs of the test set images predicting their mass
with a mean absolute error of 8\% showing thus slightly better performances
than the previous attempts reviewed \citep{Auddy_2020}.
Furthermore, the final application of the trained model to the real observation of the DS Tau disc
provided an estimation of the planet mass of $3.30 \pm 0.15 M_{jup}$ in agreement with the result, of
$3.5 \pm 1 M_{jup}$,
obtained by \citet{dstauv} employing numerical simulations. The uncertainty provided 
with our measure was computed as the root mean squared error of the models' predictions on the data of the validation
set and resulted in a significant improvement with respect to the numerical approach.

We thus showed the potential of machine learning techniques for the characterisation of 
young planets. The limited computational requirements and the ability to
directly analyse the observations are the main advantages, which could
furthermore allow the application of an adequately trained model to hypothetical large surveys
of protoplanetary discs. In a similar scenario, our approach would allow us to obtain a fast characterisation
and classification of the observed objects which then, taking advantage of this first selection,
could be further analyzed later.

\section{Future perspectives}

The dataset of synthetic images assembled in this thesis was designed with an 
eye to future developments. Many improvements can be made. First, some parameters characterising the data
assume, in all the images currently available, one of a restricted set of values failing to properly represent 
the enitre parameter space and thus limiting the possibilities of research.
For example, each synthetic image was obtained probing the simulated emission at $\SI{1.3}{\mm}$,
observations at different wavelenghts might be useful to inquire how the
performances of neural network algorithms change in relation to different observational primers.
For the same reason, the inclusion of simulations where either the scattered light or the gas molecular
emission is captured might also be interesting. In addition to that, hydrodynamical parameters such as 
the $\alpha$-viscosity, the aspect ratio and the density profiles need to be better explored 
because the interaction of these discs with planets of the same masses
could lead to the formation of different substructures as proved by the existence of \citet{kanagawa}
empirical relation.

Looking at an overview of the data collected, and used in the proof of concept, we immediately observe that
most of the images are very similar. They were in fact obtained from only two sets of hydrodynamical simulations
and were additionally filtered selecting only the discs showing well-defined gaps. 
Future developments should also include different substructures like arcs and spirals, discs 
hosting more than one planet and even binary systems in order to extend the study of machine learning methods
over all types of morphologies. The inclusion of images with gaps opened by mechanisms which do not involve the presence of
planets would also be useful to develop models trained to detect their possible presence.

In order to complete an expansion of this size we would also suggest improving the storage structure
and the supporting package with more sophisticated tools which were not used in this thesis opting for
simpler and easier to maintain solutions. First, the data could be moved in a database such as MongoDB or MySQL
provided with more advanced functions for the management and indexing of large datasets.
Then, it could be stored on an indipendent server developing a code to allow its access filtering the data directly on 
this server to avoid to the final user time and memory consuming operations. A system of this type could also be coded to
produce, when requested, new images convolved with beams of given sizes.

The absence of statistical noise in the synthetic images we collected is another important point
which should be considered in future works. Even if, 
in the proof of concept we successfully reproduced previous results, the difference with real observations 
could be relevant in other applications.
The influence of statistical errors and the background noise should thus be studied.
A possible solution consists in using \lstinline{CASA} in place of \lstinline{pymcfost} to 
include all the sources of errors in the dataset images. Alternatively,
the problem could also be attacked from the opposite side developing techniques,
maybe even using machine learning methods, to clean the original observations from these types of errors.

Finally, we would like to suggest some applications of machine learning
models which we think would be worth exploring in future researches.
Starting from the work done in this thesis with the proof of concept, similar 
models could be implemented improving the choice of the hyperparameters with 
specific algorithms designed at this purpose, in order to discover neural networks
with optimal structures for the specific problem addressed.
Without the need for any addition but requiring better performing machines, our 
dataset could be used to study the neural network susceptibility to the image resolution in terms of dimensions of
the beam size used to convolve the original one. The aim could be, for example, that of determining the worst resolution which
still allows the detection of the gap and the inferring of the planet's mass from its properties.

Different architectures of the neural network could also be explored, including convolutional
layers to account for discs symmetries and reduce errors induced by traslated or rotated images.
The output layer might also be expanded using the same network to infer more features of the disc. We imagine, 
for istance, a model that returns the planet's mass and the gas to dust ratio. Finally, the purpose of the neural network
could be changed training new models to predict different features whose determination
with traditional methods can be challenging.

\begin{comment}
-different wavelengths
-more planets e/o more stars
-observations in  gas and scatter
-understand different morphologies

-put the database on a server and do server side operations, even pymcfost or rotate
-data in database
-handle the noise

-detect the presence of planets
-up to which resolution the nn can distinguish 
-detect also gdrattio or other combinations

-convolutional nn for traslational and pos angle
-hyperparametrization algorithms

\end{comment}

\chapter*{Acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgements}




\nocite{*}
% We choose the &quot;plain&quot; reference style
\bibliography{ref}


\addcontentsline{toc}{chapter}{Bibliography}

\end{document}
